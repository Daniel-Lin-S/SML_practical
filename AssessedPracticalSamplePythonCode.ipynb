{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb32710c",
   "metadata": {},
   "source": [
    "# SML Pratical\n",
    "\n",
    "Music Genre Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5c7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94689a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec509956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and the test inputs\n",
    "X_train = pd.read_csv('X_train.csv', index_col = 0, header=[0, 1, 2]) # inputs of the training set\n",
    "y_train = pd.read_csv('y_train.csv', index_col = 0).squeeze('columns') # outputs of the training set\n",
    "X_test = pd.read_csv('X_test.csv', index_col = 0, header=[0, 1, 2]) # inputs of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbba173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_mapping = {}\n",
    "\n",
    "def transform_labels_to_numbers(labels):\n",
    "    unique_labels = set(labels)\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        class_label_mapping[label] = i\n",
    "\n",
    "    transformed_labels = [class_label_mapping[label] for label in labels]\n",
    "    \n",
    "    return transformed_labels, class_label_mapping\n",
    "\n",
    "y_train, label_mapping = transform_labels_to_numbers(y_train)\n",
    "y_train = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135c1798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"10\" halign=\"left\">chroma_cens</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">tonnetz</th>\n",
       "      <th colspan=\"7\" halign=\"left\">zcr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistics</th>\n",
       "      <th colspan=\"10\" halign=\"left\">kurtosis</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">std</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>skew</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.266585</td>\n",
       "      <td>-0.984668</td>\n",
       "      <td>-0.729823</td>\n",
       "      <td>-0.895122</td>\n",
       "      <td>2.138628</td>\n",
       "      <td>0.935209</td>\n",
       "      <td>0.104089</td>\n",
       "      <td>-0.698659</td>\n",
       "      <td>-0.736408</td>\n",
       "      <td>-0.334376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065003</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>5.743597</td>\n",
       "      <td>0.307617</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>1.976972</td>\n",
       "      <td>0.034533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.180061</td>\n",
       "      <td>0.260884</td>\n",
       "      <td>-0.069373</td>\n",
       "      <td>0.208734</td>\n",
       "      <td>-0.078855</td>\n",
       "      <td>-0.577818</td>\n",
       "      <td>0.583788</td>\n",
       "      <td>0.143781</td>\n",
       "      <td>0.291556</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087692</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.016605</td>\n",
       "      <td>64.870987</td>\n",
       "      <td>0.812988</td>\n",
       "      <td>0.082784</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>7.374503</td>\n",
       "      <td>0.074870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.692900</td>\n",
       "      <td>0.356662</td>\n",
       "      <td>0.062617</td>\n",
       "      <td>0.248280</td>\n",
       "      <td>3.470037</td>\n",
       "      <td>0.166613</td>\n",
       "      <td>0.823874</td>\n",
       "      <td>0.181112</td>\n",
       "      <td>0.551939</td>\n",
       "      <td>0.357985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132387</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>34.251705</td>\n",
       "      <td>0.850098</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>5.927942</td>\n",
       "      <td>0.117603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.243339</td>\n",
       "      <td>0.214182</td>\n",
       "      <td>-0.049026</td>\n",
       "      <td>1.456255</td>\n",
       "      <td>-0.360826</td>\n",
       "      <td>-0.875256</td>\n",
       "      <td>-0.770200</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>0.789956</td>\n",
       "      <td>0.448319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071478</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>1.364990</td>\n",
       "      <td>0.342285</td>\n",
       "      <td>0.081713</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100437</td>\n",
       "      <td>0.041754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.968576</td>\n",
       "      <td>0.309255</td>\n",
       "      <td>0.223164</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.919838</td>\n",
       "      <td>-0.111985</td>\n",
       "      <td>-1.012521</td>\n",
       "      <td>-0.665692</td>\n",
       "      <td>-0.316646</td>\n",
       "      <td>-0.264381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106220</td>\n",
       "      <td>0.023536</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>3.589230</td>\n",
       "      <td>0.322266</td>\n",
       "      <td>0.073736</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>1.210593</td>\n",
       "      <td>0.036459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>0.503490</td>\n",
       "      <td>-0.540720</td>\n",
       "      <td>-0.690117</td>\n",
       "      <td>-0.107338</td>\n",
       "      <td>-0.647856</td>\n",
       "      <td>-0.681969</td>\n",
       "      <td>-0.246245</td>\n",
       "      <td>-0.546552</td>\n",
       "      <td>0.062783</td>\n",
       "      <td>0.070393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084929</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>0.020335</td>\n",
       "      <td>4.868783</td>\n",
       "      <td>0.668945</td>\n",
       "      <td>0.076452</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>2.045856</td>\n",
       "      <td>0.084214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>-0.600597</td>\n",
       "      <td>0.406386</td>\n",
       "      <td>-0.748409</td>\n",
       "      <td>-0.316157</td>\n",
       "      <td>-0.507428</td>\n",
       "      <td>-0.054214</td>\n",
       "      <td>-0.476804</td>\n",
       "      <td>-0.373120</td>\n",
       "      <td>-0.930158</td>\n",
       "      <td>-1.080690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075407</td>\n",
       "      <td>0.014998</td>\n",
       "      <td>0.020683</td>\n",
       "      <td>7.893681</td>\n",
       "      <td>0.584961</td>\n",
       "      <td>0.076210</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.561808</td>\n",
       "      <td>0.073010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>-1.014298</td>\n",
       "      <td>-0.950744</td>\n",
       "      <td>0.618304</td>\n",
       "      <td>0.204298</td>\n",
       "      <td>-0.788411</td>\n",
       "      <td>-0.794254</td>\n",
       "      <td>-0.586847</td>\n",
       "      <td>0.099172</td>\n",
       "      <td>-0.313476</td>\n",
       "      <td>-0.523417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138591</td>\n",
       "      <td>0.024969</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>27.257378</td>\n",
       "      <td>0.373047</td>\n",
       "      <td>0.042598</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.778109</td>\n",
       "      <td>0.027813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>-0.002938</td>\n",
       "      <td>0.646034</td>\n",
       "      <td>-0.732819</td>\n",
       "      <td>1.205990</td>\n",
       "      <td>-0.898733</td>\n",
       "      <td>-0.684953</td>\n",
       "      <td>0.134642</td>\n",
       "      <td>-0.374792</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>-1.016032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>0.030371</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>431.200500</td>\n",
       "      <td>0.384277</td>\n",
       "      <td>0.025731</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>10.260160</td>\n",
       "      <td>0.006870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>-0.881917</td>\n",
       "      <td>-0.615638</td>\n",
       "      <td>0.891155</td>\n",
       "      <td>4.850262</td>\n",
       "      <td>-0.349280</td>\n",
       "      <td>-1.010980</td>\n",
       "      <td>-0.577905</td>\n",
       "      <td>-0.046604</td>\n",
       "      <td>-0.828026</td>\n",
       "      <td>-0.457468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175120</td>\n",
       "      <td>0.039703</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>13.567758</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.047689</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>2.553229</td>\n",
       "      <td>0.018049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows Ã— 518 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature    chroma_cens                                                    \\\n",
       "statistics    kurtosis                                                     \n",
       "number              01        02        03        04        05        06   \n",
       "Id                                                                         \n",
       "0            -0.266585 -0.984668 -0.729823 -0.895122  2.138628  0.935209   \n",
       "1            -0.180061  0.260884 -0.069373  0.208734 -0.078855 -0.577818   \n",
       "2            -0.692900  0.356662  0.062617  0.248280  3.470037  0.166613   \n",
       "3             0.243339  0.214182 -0.049026  1.456255 -0.360826 -0.875256   \n",
       "4            -0.968576  0.309255  0.223164  0.160960  0.919838 -0.111985   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "5995          0.503490 -0.540720 -0.690117 -0.107338 -0.647856 -0.681969   \n",
       "5996         -0.600597  0.406386 -0.748409 -0.316157 -0.507428 -0.054214   \n",
       "5997         -1.014298 -0.950744  0.618304  0.204298 -0.788411 -0.794254   \n",
       "5998         -0.002938  0.646034 -0.732819  1.205990 -0.898733 -0.684953   \n",
       "5999         -0.881917 -0.615638  0.891155  4.850262 -0.349280 -1.010980   \n",
       "\n",
       "feature                                             ...   tonnetz            \\\n",
       "statistics                                          ...       std             \n",
       "number            07        08        09        10  ...        04        05   \n",
       "Id                                                  ...                       \n",
       "0           0.104089 -0.698659 -0.736408 -0.334376  ...  0.065003  0.016522   \n",
       "1           0.583788  0.143781  0.291556  0.007314  ...  0.087692  0.016355   \n",
       "2           0.823874  0.181112  0.551939  0.357985  ...  0.132387  0.025847   \n",
       "3          -0.770200  0.315500  0.789956  0.448319  ...  0.071478  0.019166   \n",
       "4          -1.012521 -0.665692 -0.316646 -0.264381  ...  0.106220  0.023536   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "5995       -0.246245 -0.546552  0.062783  0.070393  ...  0.084929  0.017250   \n",
       "5996       -0.476804 -0.373120 -0.930158 -1.080690  ...  0.075407  0.014998   \n",
       "5997       -0.586847  0.099172 -0.313476 -0.523417  ...  0.138591  0.024969   \n",
       "5998        0.134642 -0.374792 -0.019524 -1.016032  ...  0.137695  0.030371   \n",
       "5999       -0.577905 -0.046604 -0.828026 -0.457468  ...  0.175120  0.039703   \n",
       "\n",
       "feature                      zcr                                          \\\n",
       "statistics              kurtosis       max      mean    median       min   \n",
       "number            06          01        01        01        01        01   \n",
       "Id                                                                         \n",
       "0           0.015776    5.743597  0.307617  0.051370  0.042480  0.002441   \n",
       "1           0.016605   64.870987  0.812988  0.082784  0.069824  0.003906   \n",
       "2           0.023922   34.251705  0.850098  0.058200  0.036621  0.010254   \n",
       "3           0.025535    1.364990  0.342285  0.081713  0.075195  0.000000   \n",
       "4           0.019742    3.589230  0.322266  0.073736  0.069336  0.004395   \n",
       "...              ...         ...       ...       ...       ...       ...   \n",
       "5995        0.020335    4.868783  0.668945  0.076452  0.044434  0.001465   \n",
       "5996        0.020683    7.893681  0.584961  0.076210  0.048340  0.000000   \n",
       "5997        0.023658   27.257378  0.373047  0.042598  0.037598  0.000000   \n",
       "5998        0.029970  431.200500  0.384277  0.025731  0.025391  0.008301   \n",
       "5999        0.025094   13.567758  0.233398  0.047689  0.044922  0.015625   \n",
       "\n",
       "feature                          \n",
       "statistics       skew       std  \n",
       "number             01        01  \n",
       "Id                               \n",
       "0            1.976972  0.034533  \n",
       "1            7.374503  0.074870  \n",
       "2            5.927942  0.117603  \n",
       "3            1.100437  0.041754  \n",
       "4            1.210593  0.036459  \n",
       "...               ...       ...  \n",
       "5995         2.045856  0.084214  \n",
       "5996         2.561808  0.073010  \n",
       "5997         3.778109  0.027813  \n",
       "5998        10.260160  0.006870  \n",
       "5999         2.553229  0.018049  \n",
       "\n",
       "[6000 rows x 518 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of rows and columns(attributes)\n",
    "n, p = np.shape(X_train)\n",
    "# Entries (i,j) correspond to the j'th dimension of the observation i\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"unique features: {X_train.columns.get_level_values('feature').unique().tolist()}\")\n",
    "print(f\"statistics used: {X_train.columns.get_level_values('statistics').unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee583d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  (may not be useful) plot correlations for each set of statistics\n",
    "\n",
    "statistics = X_train.columns.get_level_values('statistics').unique()\n",
    "\n",
    "for statistic in statistics:\n",
    "    # obtain the columns for each feature\n",
    "    cols = [col for col in X_train if col[1] == statistic]\n",
    "    # find the correlation matrix\n",
    "    corr = X_train[cols].corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 11))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    plt.title(f'Correlation Heatmap (for {statistic})', fontsize = 25)\n",
    "    plt.xticks(fontsize = 10)\n",
    "    plt.yticks(fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train contains the true class:  Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock\n",
    "classes = np.unique(y_train)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test is the array of test inputs, of the same format as X_train. The objective is to predict the class (Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock) of the output\n",
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f32c045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "### normalise the training set and test set together ###\n",
    "X = pd.concat([X_train, X_test], ignore_index=True)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
    "X_train_scaled = X_scaled.iloc[:6000,:]\n",
    "X_test_scaled = X_scaled.iloc[6000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4253d",
   "metadata": {},
   "source": [
    "## PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390646ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### use PCA to reduce the dimension ###\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=p)\n",
    "# find the principal compoennts\n",
    "pc = pd.DataFrame(data = pca.fit_transform(X_train_scaled), columns = [f'PC {i}' for i in range(1, p+1)])\n",
    "\n",
    "# concatenate labels \n",
    "Df_PCA = pd.concat([pc, y_train], axis=1)\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5efff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 101), explained_variances[:100])\n",
    "plt.title('explained variances by principal components')\n",
    "plt.xlabel('PC index')\n",
    "plt.ylabel('ratio of explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fe7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_PCA = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253bb33",
   "metadata": {},
   "source": [
    "elbow method: take around 20 PCs as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first two principal components (useless plot, messy)\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "sns.scatterplot(x='PC 1', y='PC 2', hue=Df_PCA['Genre'], data=Df_PCA.iloc[:, :2], alpha=0.5)\n",
    "\n",
    "\n",
    "plt.title('Plot of first two components, with the genre represented by colour', fontsize=17)\n",
    "plt.xlabel('first principal component', fontsize=14)\n",
    "plt.xlabel('second principal component', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96abcf",
   "metadata": {},
   "source": [
    "## Classical Training Models\n",
    "Naive Bayes, two-layer perceptron, linear SVM, kernel SVM, random forests (and with gradient boosting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8af683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try Various Machien Learning Algorithms ###\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from xgboost import plot_tree, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### casual trainings with no tuning ###\n",
    "nb =  GaussianNB()\n",
    "sgd = SGDClassifier(max_iter=4000)\n",
    "tree = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20)\n",
    "svm = SVC(decision_function_shape=\"ovo\")\n",
    "lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(200, 10), random_state=1)\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.04)\n",
    "xgbrf = XGBRFClassifier(objective= 'multi:softmax')\n",
    "\n",
    "algorithms = {\n",
    "    'naive_Bayes': nb,\n",
    "    'SGD' : sgd,\n",
    "    'Decision_tree': tree,\n",
    "    'random_forest': rf,\n",
    "    'SVM': svm,\n",
    "    'logistic_regression': lg,\n",
    "    'neural network': nn,\n",
    "    'cross-gradient boosting tree': xgb,\n",
    "    'cross-gradient boosting': xgbrf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c541f1",
   "metadata": {},
   "source": [
    "### PCA for dimension-reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "281d7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(X_train_scaled, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "## use PCA to reduce dimension. n = 20\n",
    "pca = PCA(n_components=n_PCA)\n",
    "X_t_PC = pd.DataFrame(data = pca.fit_transform(X_t), columns = [f'PC {i}' for i in range(1, n_PCA+1)])\n",
    "pca = PCA(n_components=n_PCA)\n",
    "X_val_PC = pd.DataFrame(data = pca.fit_transform(X_val), columns = [f'PC {i}' for i in range(1, n_PCA+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd8e5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_PCA(model, name):\n",
    "    model.fit(X_t_PC, y_t)\n",
    "    y_pred = model.predict(X_val_PC)\n",
    "    print('Validation Accuracy', name, ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f587e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy naive_Bayes : 0.26778 \n",
      "\n",
      "Validation Accuracy SGD : 0.18611 \n",
      "\n",
      "Validation Accuracy Decision_tree : 0.16611 \n",
      "\n",
      "Validation Accuracy random_forest : 0.22778 \n",
      "\n",
      "Validation Accuracy SVM : 0.23167 \n",
      "\n",
      "Validation Accuracy logistic_regression : 0.20778 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linyuhang/mambaforge/envs/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy neural network : 0.19889 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting tree : 0.23222 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting : 0.24167 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, algorithm in algorithms.items():\n",
    "    model_PCA(algorithm, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ade29",
   "metadata": {},
   "source": [
    "### Fisher's LDA for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8709f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.54833 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Use LDA classifier ###\n",
    "LDAclassifier = LinearDiscriminantAnalysis(n_components=7) # 7 classes in total \n",
    "LDAclassifier.fit(X_t, y_t)\n",
    "y_pred = LDAclassifier.predict(X_val)\n",
    "print('Validation Accuracy', ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "096b10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use LDA to reduce dimension, and apply other algorithms ###\n",
    "\n",
    "def model_LDA(model, name):\n",
    "    \"\"\"training based on LDA for dimension-reduction\"\"\"\n",
    "    # obtain LDA components for other algorithms\n",
    "    LDA = LinearDiscriminantAnalysis()\n",
    "    LDA.fit(X_t, y_t)\n",
    "    X_t_LDA = LDA.transform(X_t)\n",
    "    X_val_LDA = LDA.transform(X_val)      \n",
    "    model.fit(X_t_LDA, y_t)\n",
    "    y_pred = model.predict(X_val_LDA)\n",
    "    print('Validation Accuracy', name, ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e6862a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy naive_Bayes : 0.55833 \n",
      "\n",
      "Validation Accuracy SGD : 0.53278 \n",
      "\n",
      "Validation Accuracy Decision_tree : 0.44833 \n",
      "\n",
      "Validation Accuracy random_forest : 0.55167 \n",
      "\n",
      "Validation Accuracy SVM : 0.55722 \n",
      "\n",
      "Validation Accuracy logistic_regression : 0.54611 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linyuhang/mambaforge/envs/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy neural network : 0.50556 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting tree : 0.54556 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting : 0.54333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, algorithm in algorithms.items():\n",
    "    model_LDA(algorithm, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bcee2",
   "metadata": {},
   "source": [
    "Conclusion: LDA is better than PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33bb35",
   "metadata": {},
   "source": [
    "## Cross-validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d204196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f48ed185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LDA to reduce dimensions\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train_scaled, y_train)\n",
    "X_train_LDA = LDA.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82f47920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD tuning: regularisation strength, penalty and loss \n",
    "sgd = SGDClassifier(max_iter=5000)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'loss': list(sgd.loss_functions)\n",
    "}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    grid_SGD = GridSearchCV(sgd, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_SGD.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ea664e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6898333333333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(grid_SGD.best_params_)\n",
    "grid_SGD.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaef554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest tuning:  \n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100], # number of trees\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [8, 10, 20], # minimum size of node for splitting\n",
    "    'min_samples_leaf': [2, 4, 6],  # minimum leaf size \n",
    "    'max_features': ['auto', 'sqrt', 'log2', None] # number of features used in each tree\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_RF = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_RF.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513636d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_RF.best_params_)\n",
    "grid_RF.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tuning logistic regression\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], # regularisation strength\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cg']  # solver for optimisation\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_logit = GridSearchCV(logreg_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_logit.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e44de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_logit.best_params_)\n",
    "grid_logit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95025b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunig gradient boosting random forest\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2], # step size shrinkage used in each boosting iteration\n",
    "    'n_estimators': [50, 100], # number of trees\n",
    "    'max_depth': [5, 7, 10], # max tree depth\n",
    "    'subsample': [0.6, 0.8], # fraction of samples used for fitting each tree\n",
    "    'colsample_bynode': [0.6, 0.8], # fraction of features used for fitting each tree\n",
    "    'gamma': [0, 0.1, 0.2], # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "}\n",
    "\n",
    "# Create the XGBRFClassifier\n",
    "xgbrf_classifier = XGBRFClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_xgb = GridSearchCV(xgbrf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_xgb.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beae048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_xgb.best_params_)\n",
    "grid_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20238d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tuning Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Define the parameter grid for AdaBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'base_estimator__max_depth': [2, 4, 6] \n",
    "}\n",
    "\n",
    "# Create the AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(base_estimator=base_estimator)\n",
    "\n",
    "# grid search\n",
    "grid_ada = GridSearchCV(adaboost, param_grid, cv=5, scoring='accuracy')\n",
    "grid_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_ada.best_params_)\n",
    "grid_ada.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46597438",
   "metadata": {},
   "source": [
    "## MLP training using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88fa4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b98d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LDA reduced data set\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train_LDA, y_train, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee6dbad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "132/132 [==============================] - 1s 1ms/step - loss: 2.0694 - accuracy: 0.1640 - val_loss: 1.9718 - val_accuracy: 0.4139\n",
      "Epoch 2/500\n",
      "132/132 [==============================] - 0s 864us/step - loss: 1.9735 - accuracy: 0.2171 - val_loss: 1.7780 - val_accuracy: 0.5328\n",
      "Epoch 3/500\n",
      "132/132 [==============================] - 0s 904us/step - loss: 1.8668 - accuracy: 0.2748 - val_loss: 1.5879 - val_accuracy: 0.5800\n",
      "Epoch 4/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.7547 - accuracy: 0.3440 - val_loss: 1.4177 - val_accuracy: 0.6228\n",
      "Epoch 5/500\n",
      "132/132 [==============================] - 0s 893us/step - loss: 1.6832 - accuracy: 0.3840 - val_loss: 1.3355 - val_accuracy: 0.6289\n",
      "Epoch 6/500\n",
      "132/132 [==============================] - 0s 893us/step - loss: 1.6465 - accuracy: 0.4107 - val_loss: 1.2952 - val_accuracy: 0.6472\n",
      "Epoch 7/500\n",
      "132/132 [==============================] - 0s 863us/step - loss: 1.6110 - accuracy: 0.4262 - val_loss: 1.2457 - val_accuracy: 0.6533\n",
      "Epoch 8/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.5821 - accuracy: 0.4393 - val_loss: 1.1998 - val_accuracy: 0.6589\n",
      "Epoch 9/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.5547 - accuracy: 0.4598 - val_loss: 1.2079 - val_accuracy: 0.6656\n",
      "Epoch 10/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.5260 - accuracy: 0.4752 - val_loss: 1.1691 - val_accuracy: 0.6711\n",
      "Epoch 11/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.5004 - accuracy: 0.4764 - val_loss: 1.1574 - val_accuracy: 0.6756\n",
      "Epoch 12/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.4797 - accuracy: 0.4852 - val_loss: 1.1301 - val_accuracy: 0.6822\n",
      "Epoch 13/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.4920 - accuracy: 0.4910 - val_loss: 1.1384 - val_accuracy: 0.6850\n",
      "Epoch 14/500\n",
      "132/132 [==============================] - 0s 856us/step - loss: 1.4772 - accuracy: 0.5012 - val_loss: 1.1305 - val_accuracy: 0.6778\n",
      "Epoch 15/500\n",
      "132/132 [==============================] - 0s 916us/step - loss: 1.4771 - accuracy: 0.5145 - val_loss: 1.1280 - val_accuracy: 0.6756\n",
      "Epoch 16/500\n",
      "132/132 [==============================] - 0s 894us/step - loss: 1.4559 - accuracy: 0.5150 - val_loss: 1.1229 - val_accuracy: 0.6800\n",
      "Epoch 17/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.4774 - accuracy: 0.5010 - val_loss: 1.1233 - val_accuracy: 0.6850\n",
      "Epoch 18/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.4628 - accuracy: 0.5138 - val_loss: 1.1152 - val_accuracy: 0.6778\n",
      "Epoch 19/500\n",
      "132/132 [==============================] - 0s 866us/step - loss: 1.4656 - accuracy: 0.5086 - val_loss: 1.1132 - val_accuracy: 0.6822\n",
      "Epoch 20/500\n",
      "132/132 [==============================] - 0s 878us/step - loss: 1.4348 - accuracy: 0.5283 - val_loss: 1.0996 - val_accuracy: 0.6828\n",
      "Epoch 21/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.4412 - accuracy: 0.5186 - val_loss: 1.1070 - val_accuracy: 0.6844\n",
      "Epoch 22/500\n",
      "132/132 [==============================] - 0s 965us/step - loss: 1.4484 - accuracy: 0.5238 - val_loss: 1.1066 - val_accuracy: 0.6839\n",
      "Epoch 23/500\n",
      "132/132 [==============================] - 0s 891us/step - loss: 1.4321 - accuracy: 0.5190 - val_loss: 1.1041 - val_accuracy: 0.6867\n",
      "Epoch 24/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.4331 - accuracy: 0.5298 - val_loss: 1.0966 - val_accuracy: 0.6878\n",
      "Epoch 25/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.4058 - accuracy: 0.5436 - val_loss: 1.0846 - val_accuracy: 0.6856\n",
      "Epoch 26/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3985 - accuracy: 0.5474 - val_loss: 1.0968 - val_accuracy: 0.6811\n",
      "Epoch 27/500\n",
      "132/132 [==============================] - 0s 827us/step - loss: 1.4137 - accuracy: 0.5410 - val_loss: 1.0943 - val_accuracy: 0.6783\n",
      "Epoch 28/500\n",
      "132/132 [==============================] - 0s 864us/step - loss: 1.3973 - accuracy: 0.5462 - val_loss: 1.0886 - val_accuracy: 0.6850\n",
      "Epoch 29/500\n",
      "132/132 [==============================] - 0s 860us/step - loss: 1.4062 - accuracy: 0.5462 - val_loss: 1.0890 - val_accuracy: 0.6822\n",
      "Epoch 30/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.3981 - accuracy: 0.5512 - val_loss: 1.0915 - val_accuracy: 0.6894\n",
      "Epoch 31/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.3809 - accuracy: 0.5552 - val_loss: 1.0785 - val_accuracy: 0.6928\n",
      "Epoch 32/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3951 - accuracy: 0.5414 - val_loss: 1.0878 - val_accuracy: 0.6906\n",
      "Epoch 33/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.3970 - accuracy: 0.5400 - val_loss: 1.0772 - val_accuracy: 0.6833\n",
      "Epoch 34/500\n",
      "132/132 [==============================] - 0s 821us/step - loss: 1.3851 - accuracy: 0.5433 - val_loss: 1.0778 - val_accuracy: 0.6889\n",
      "Epoch 35/500\n",
      "132/132 [==============================] - 0s 820us/step - loss: 1.4096 - accuracy: 0.5388 - val_loss: 1.0800 - val_accuracy: 0.6850\n",
      "Epoch 36/500\n",
      "132/132 [==============================] - 0s 820us/step - loss: 1.3726 - accuracy: 0.5545 - val_loss: 1.0744 - val_accuracy: 0.6950\n",
      "Epoch 37/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.3776 - accuracy: 0.5526 - val_loss: 1.0642 - val_accuracy: 0.6939\n",
      "Epoch 38/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.3589 - accuracy: 0.5607 - val_loss: 1.0691 - val_accuracy: 0.6956\n",
      "Epoch 39/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.3717 - accuracy: 0.5538 - val_loss: 1.0612 - val_accuracy: 0.6922\n",
      "Epoch 40/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3619 - accuracy: 0.5538 - val_loss: 1.0699 - val_accuracy: 0.6833\n",
      "Epoch 41/500\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.3588 - accuracy: 0.5493 - val_loss: 1.0612 - val_accuracy: 0.6944\n",
      "Epoch 42/500\n",
      "132/132 [==============================] - 0s 816us/step - loss: 1.3807 - accuracy: 0.5679 - val_loss: 1.0661 - val_accuracy: 0.6939\n",
      "Epoch 43/500\n",
      "132/132 [==============================] - 0s 811us/step - loss: 1.3919 - accuracy: 0.5440 - val_loss: 1.0621 - val_accuracy: 0.6950\n",
      "Epoch 44/500\n",
      "132/132 [==============================] - 0s 822us/step - loss: 1.3654 - accuracy: 0.5583 - val_loss: 1.0633 - val_accuracy: 0.6917\n",
      "Epoch 45/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.3473 - accuracy: 0.5555 - val_loss: 1.0553 - val_accuracy: 0.6939\n",
      "Epoch 46/500\n",
      "132/132 [==============================] - 0s 821us/step - loss: 1.3389 - accuracy: 0.5705 - val_loss: 1.0534 - val_accuracy: 0.6939\n",
      "Epoch 47/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3695 - accuracy: 0.5631 - val_loss: 1.0659 - val_accuracy: 0.6922\n",
      "Epoch 48/500\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.3577 - accuracy: 0.5586 - val_loss: 1.0578 - val_accuracy: 0.6922\n",
      "Epoch 49/500\n",
      "132/132 [==============================] - 0s 811us/step - loss: 1.3417 - accuracy: 0.5698 - val_loss: 1.0543 - val_accuracy: 0.6928\n",
      "Epoch 50/500\n",
      "132/132 [==============================] - 0s 813us/step - loss: 1.3755 - accuracy: 0.5645 - val_loss: 1.0609 - val_accuracy: 0.6933\n",
      "Epoch 51/500\n",
      "132/132 [==============================] - 0s 822us/step - loss: 1.3542 - accuracy: 0.5648 - val_loss: 1.0565 - val_accuracy: 0.6906\n",
      "Epoch 52/500\n",
      "132/132 [==============================] - 0s 814us/step - loss: 1.3253 - accuracy: 0.5721 - val_loss: 1.0465 - val_accuracy: 0.6928\n",
      "Epoch 53/500\n",
      "132/132 [==============================] - 0s 814us/step - loss: 1.3454 - accuracy: 0.5624 - val_loss: 1.0520 - val_accuracy: 0.6889\n",
      "Epoch 54/500\n",
      "132/132 [==============================] - 0s 817us/step - loss: 1.3526 - accuracy: 0.5664 - val_loss: 1.0644 - val_accuracy: 0.6867\n",
      "Epoch 55/500\n",
      "132/132 [==============================] - 0s 816us/step - loss: 1.3416 - accuracy: 0.5605 - val_loss: 1.0563 - val_accuracy: 0.6911\n",
      "Epoch 56/500\n",
      "132/132 [==============================] - 0s 810us/step - loss: 1.3498 - accuracy: 0.5667 - val_loss: 1.0529 - val_accuracy: 0.6894\n",
      "Epoch 57/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3790 - accuracy: 0.5590 - val_loss: 1.0651 - val_accuracy: 0.6917\n",
      "Epoch 58/500\n",
      "132/132 [==============================] - 0s 807us/step - loss: 1.3496 - accuracy: 0.5593 - val_loss: 1.0590 - val_accuracy: 0.6844\n",
      "Epoch 59/500\n",
      "132/132 [==============================] - 0s 811us/step - loss: 1.3588 - accuracy: 0.5729 - val_loss: 1.0549 - val_accuracy: 0.6894\n",
      "Epoch 60/500\n",
      "132/132 [==============================] - 0s 821us/step - loss: 1.3377 - accuracy: 0.5690 - val_loss: 1.0580 - val_accuracy: 0.6950\n",
      "Epoch 61/500\n",
      "132/132 [==============================] - 0s 809us/step - loss: 1.3688 - accuracy: 0.5605 - val_loss: 1.0643 - val_accuracy: 0.6894\n",
      "Epoch 62/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.3369 - accuracy: 0.5645 - val_loss: 1.0592 - val_accuracy: 0.6861\n",
      "Epoch 63/500\n",
      "132/132 [==============================] - 0s 820us/step - loss: 1.3250 - accuracy: 0.5719 - val_loss: 1.0576 - val_accuracy: 0.6850\n",
      "Epoch 64/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.3530 - accuracy: 0.5624 - val_loss: 1.0504 - val_accuracy: 0.6883\n",
      "Epoch 65/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.3279 - accuracy: 0.5645 - val_loss: 1.0592 - val_accuracy: 0.6889\n",
      "Epoch 66/500\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.3373 - accuracy: 0.5660 - val_loss: 1.0546 - val_accuracy: 0.6911\n",
      "Epoch 67/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.3445 - accuracy: 0.5757 - val_loss: 1.0542 - val_accuracy: 0.6906\n",
      "Epoch 68/500\n",
      "132/132 [==============================] - 0s 823us/step - loss: 1.3537 - accuracy: 0.5712 - val_loss: 1.0504 - val_accuracy: 0.6900\n",
      "Epoch 69/500\n",
      "132/132 [==============================] - 0s 825us/step - loss: 1.3371 - accuracy: 0.5757 - val_loss: 1.0647 - val_accuracy: 0.6806\n",
      "Epoch 70/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.3471 - accuracy: 0.5655 - val_loss: 1.0601 - val_accuracy: 0.6889\n",
      "Epoch 71/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.3473 - accuracy: 0.5748 - val_loss: 1.0595 - val_accuracy: 0.6883\n",
      "Epoch 72/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.3304 - accuracy: 0.5714 - val_loss: 1.0499 - val_accuracy: 0.6894\n",
      "Epoch 73/500\n",
      "132/132 [==============================] - 0s 860us/step - loss: 1.3557 - accuracy: 0.5631 - val_loss: 1.0600 - val_accuracy: 0.6850\n",
      "Epoch 74/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3384 - accuracy: 0.5655 - val_loss: 1.0593 - val_accuracy: 0.6872\n",
      "Epoch 75/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.3055 - accuracy: 0.5795 - val_loss: 1.0541 - val_accuracy: 0.6883\n",
      "Epoch 76/500\n",
      "132/132 [==============================] - 0s 820us/step - loss: 1.3406 - accuracy: 0.5712 - val_loss: 1.0533 - val_accuracy: 0.6906\n",
      "Epoch 77/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.3282 - accuracy: 0.5719 - val_loss: 1.0496 - val_accuracy: 0.6900\n",
      "Epoch 78/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.3381 - accuracy: 0.5726 - val_loss: 1.0548 - val_accuracy: 0.6900\n",
      "Epoch 79/500\n",
      "132/132 [==============================] - 0s 825us/step - loss: 1.3092 - accuracy: 0.5774 - val_loss: 1.0508 - val_accuracy: 0.6883\n",
      "Epoch 80/500\n",
      "132/132 [==============================] - 0s 820us/step - loss: 1.3361 - accuracy: 0.5731 - val_loss: 1.0555 - val_accuracy: 0.6872\n",
      "Epoch 81/500\n",
      "132/132 [==============================] - 0s 814us/step - loss: 1.3159 - accuracy: 0.5776 - val_loss: 1.0511 - val_accuracy: 0.6917\n",
      "Epoch 82/500\n",
      "132/132 [==============================] - 0s 817us/step - loss: 1.3384 - accuracy: 0.5690 - val_loss: 1.0500 - val_accuracy: 0.6911\n",
      "Epoch 83/500\n",
      "132/132 [==============================] - 0s 822us/step - loss: 1.3157 - accuracy: 0.5743 - val_loss: 1.0494 - val_accuracy: 0.6956\n",
      "Epoch 84/500\n",
      "132/132 [==============================] - 0s 804us/step - loss: 1.3308 - accuracy: 0.5721 - val_loss: 1.0532 - val_accuracy: 0.6867\n",
      "Epoch 85/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.3172 - accuracy: 0.5779 - val_loss: 1.0511 - val_accuracy: 0.6894\n",
      "Epoch 86/500\n",
      "132/132 [==============================] - 0s 821us/step - loss: 1.3085 - accuracy: 0.5788 - val_loss: 1.0456 - val_accuracy: 0.6900\n",
      "Epoch 87/500\n",
      "132/132 [==============================] - 0s 825us/step - loss: 1.3335 - accuracy: 0.5745 - val_loss: 1.0474 - val_accuracy: 0.6872\n",
      "Epoch 88/500\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.3358 - accuracy: 0.5643 - val_loss: 1.0537 - val_accuracy: 0.6967\n",
      "Epoch 89/500\n",
      "132/132 [==============================] - 0s 986us/step - loss: 1.3108 - accuracy: 0.5693 - val_loss: 1.0463 - val_accuracy: 0.6944\n",
      "Epoch 90/500\n",
      "132/132 [==============================] - 0s 881us/step - loss: 1.3219 - accuracy: 0.5800 - val_loss: 1.0570 - val_accuracy: 0.6878\n",
      "Epoch 91/500\n",
      "132/132 [==============================] - 0s 818us/step - loss: 1.3052 - accuracy: 0.5793 - val_loss: 1.0452 - val_accuracy: 0.6917\n",
      "Epoch 92/500\n",
      "132/132 [==============================] - 0s 817us/step - loss: 1.2988 - accuracy: 0.5840 - val_loss: 1.0441 - val_accuracy: 0.6933\n",
      "Epoch 93/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.3291 - accuracy: 0.5674 - val_loss: 1.0541 - val_accuracy: 0.6894\n",
      "Epoch 94/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3342 - accuracy: 0.5779 - val_loss: 1.0585 - val_accuracy: 0.6861\n",
      "Epoch 95/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.3253 - accuracy: 0.5733 - val_loss: 1.0542 - val_accuracy: 0.6867\n",
      "Epoch 96/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.3435 - accuracy: 0.5757 - val_loss: 1.0542 - val_accuracy: 0.6850\n",
      "Epoch 97/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.3098 - accuracy: 0.5679 - val_loss: 1.0457 - val_accuracy: 0.6933\n",
      "Epoch 98/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.3253 - accuracy: 0.5676 - val_loss: 1.0469 - val_accuracy: 0.6900\n",
      "Epoch 99/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.3101 - accuracy: 0.5802 - val_loss: 1.0506 - val_accuracy: 0.6872\n",
      "Epoch 100/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.3228 - accuracy: 0.5693 - val_loss: 1.0467 - val_accuracy: 0.6872\n",
      "Epoch 101/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3170 - accuracy: 0.5774 - val_loss: 1.0587 - val_accuracy: 0.6839\n",
      "Epoch 102/500\n",
      "132/132 [==============================] - 0s 827us/step - loss: 1.3102 - accuracy: 0.5771 - val_loss: 1.0473 - val_accuracy: 0.6944\n",
      "Epoch 103/500\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.3052 - accuracy: 0.5821 - val_loss: 1.0477 - val_accuracy: 0.6944\n",
      "Epoch 104/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3035 - accuracy: 0.5807 - val_loss: 1.0470 - val_accuracy: 0.6944\n",
      "Epoch 105/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3176 - accuracy: 0.5810 - val_loss: 1.0461 - val_accuracy: 0.6956\n",
      "Epoch 106/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2744 - accuracy: 0.5848 - val_loss: 1.0398 - val_accuracy: 0.6933\n",
      "Epoch 107/500\n",
      "132/132 [==============================] - 0s 815us/step - loss: 1.3221 - accuracy: 0.5760 - val_loss: 1.0425 - val_accuracy: 0.6883\n",
      "Epoch 108/500\n",
      "132/132 [==============================] - 0s 807us/step - loss: 1.3035 - accuracy: 0.5836 - val_loss: 1.0412 - val_accuracy: 0.6950\n",
      "Epoch 109/500\n",
      "132/132 [==============================] - 0s 817us/step - loss: 1.3091 - accuracy: 0.5662 - val_loss: 1.0496 - val_accuracy: 0.6883\n",
      "Epoch 110/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.3332 - accuracy: 0.5705 - val_loss: 1.0537 - val_accuracy: 0.6906\n",
      "Epoch 111/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.3078 - accuracy: 0.5683 - val_loss: 1.0471 - val_accuracy: 0.6917\n",
      "Epoch 112/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.3034 - accuracy: 0.5855 - val_loss: 1.0494 - val_accuracy: 0.6922\n",
      "Epoch 113/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.3011 - accuracy: 0.5810 - val_loss: 1.0462 - val_accuracy: 0.6894\n",
      "Epoch 114/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.2917 - accuracy: 0.5848 - val_loss: 1.0456 - val_accuracy: 0.6906\n",
      "Epoch 115/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2932 - accuracy: 0.5829 - val_loss: 1.0462 - val_accuracy: 0.6917\n",
      "Epoch 116/500\n",
      "132/132 [==============================] - 0s 827us/step - loss: 1.3072 - accuracy: 0.5879 - val_loss: 1.0450 - val_accuracy: 0.6994\n",
      "Epoch 117/500\n",
      "132/132 [==============================] - 0s 821us/step - loss: 1.3283 - accuracy: 0.5812 - val_loss: 1.0485 - val_accuracy: 0.6989\n",
      "Epoch 118/500\n",
      "132/132 [==============================] - 0s 817us/step - loss: 1.3060 - accuracy: 0.5800 - val_loss: 1.0497 - val_accuracy: 0.6972\n",
      "Epoch 119/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3230 - accuracy: 0.5831 - val_loss: 1.0503 - val_accuracy: 0.6950\n",
      "Epoch 120/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2924 - accuracy: 0.5857 - val_loss: 1.0422 - val_accuracy: 0.6972\n",
      "Epoch 121/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.3008 - accuracy: 0.5821 - val_loss: 1.0452 - val_accuracy: 0.6939\n",
      "Epoch 122/500\n",
      "132/132 [==============================] - 0s 815us/step - loss: 1.3120 - accuracy: 0.5779 - val_loss: 1.0516 - val_accuracy: 0.6922\n",
      "Epoch 123/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.3131 - accuracy: 0.5767 - val_loss: 1.0504 - val_accuracy: 0.6922\n",
      "Epoch 124/500\n",
      "132/132 [==============================] - 0s 822us/step - loss: 1.2955 - accuracy: 0.5836 - val_loss: 1.0447 - val_accuracy: 0.6911\n",
      "Epoch 125/500\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.2984 - accuracy: 0.5800 - val_loss: 1.0504 - val_accuracy: 0.6894\n",
      "Epoch 126/500\n",
      "132/132 [==============================] - 0s 825us/step - loss: 1.3069 - accuracy: 0.5686 - val_loss: 1.0490 - val_accuracy: 0.6889\n",
      "Epoch 127/500\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.3071 - accuracy: 0.5805 - val_loss: 1.0486 - val_accuracy: 0.6878\n",
      "Epoch 128/500\n",
      "132/132 [==============================] - 0s 820us/step - loss: 1.3121 - accuracy: 0.5845 - val_loss: 1.0492 - val_accuracy: 0.6850\n",
      "Epoch 129/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.3182 - accuracy: 0.5845 - val_loss: 1.0547 - val_accuracy: 0.6872\n",
      "Epoch 130/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.3278 - accuracy: 0.5745 - val_loss: 1.0514 - val_accuracy: 0.6900\n",
      "Epoch 131/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.3178 - accuracy: 0.5755 - val_loss: 1.0462 - val_accuracy: 0.6978\n",
      "Epoch 132/500\n",
      "132/132 [==============================] - 0s 827us/step - loss: 1.3002 - accuracy: 0.5757 - val_loss: 1.0485 - val_accuracy: 0.6878\n",
      "Epoch 133/500\n",
      "132/132 [==============================] - 0s 822us/step - loss: 1.2927 - accuracy: 0.5788 - val_loss: 1.0433 - val_accuracy: 0.6911\n",
      "Epoch 134/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3208 - accuracy: 0.5838 - val_loss: 1.0510 - val_accuracy: 0.6928\n",
      "Epoch 135/500\n",
      "132/132 [==============================] - 0s 893us/step - loss: 1.2888 - accuracy: 0.5819 - val_loss: 1.0395 - val_accuracy: 0.6939\n",
      "Epoch 136/500\n",
      "132/132 [==============================] - 0s 895us/step - loss: 1.3073 - accuracy: 0.5812 - val_loss: 1.0461 - val_accuracy: 0.6878\n",
      "Epoch 137/500\n",
      "132/132 [==============================] - 0s 862us/step - loss: 1.2939 - accuracy: 0.5740 - val_loss: 1.0490 - val_accuracy: 0.6894\n",
      "Epoch 138/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2948 - accuracy: 0.5857 - val_loss: 1.0473 - val_accuracy: 0.6878\n",
      "Epoch 139/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.3202 - accuracy: 0.5664 - val_loss: 1.0485 - val_accuracy: 0.6939\n",
      "Epoch 140/500\n",
      "132/132 [==============================] - 0s 884us/step - loss: 1.2777 - accuracy: 0.5826 - val_loss: 1.0417 - val_accuracy: 0.6878\n",
      "Epoch 141/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.3240 - accuracy: 0.5714 - val_loss: 1.0460 - val_accuracy: 0.6889\n",
      "Epoch 142/500\n",
      "132/132 [==============================] - 0s 832us/step - loss: 1.3003 - accuracy: 0.5779 - val_loss: 1.0458 - val_accuracy: 0.6906\n",
      "Epoch 143/500\n",
      "132/132 [==============================] - 0s 862us/step - loss: 1.3075 - accuracy: 0.5852 - val_loss: 1.0478 - val_accuracy: 0.6822\n",
      "Epoch 144/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2873 - accuracy: 0.5729 - val_loss: 1.0433 - val_accuracy: 0.6861\n",
      "Epoch 145/500\n",
      "132/132 [==============================] - 0s 852us/step - loss: 1.3042 - accuracy: 0.5683 - val_loss: 1.0489 - val_accuracy: 0.6894\n",
      "Epoch 146/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2971 - accuracy: 0.5800 - val_loss: 1.0526 - val_accuracy: 0.6878\n",
      "Epoch 147/500\n",
      "132/132 [==============================] - 0s 877us/step - loss: 1.3122 - accuracy: 0.5788 - val_loss: 1.0493 - val_accuracy: 0.6850\n",
      "Epoch 148/500\n",
      "132/132 [==============================] - 0s 861us/step - loss: 1.2859 - accuracy: 0.5771 - val_loss: 1.0461 - val_accuracy: 0.6933\n",
      "Epoch 149/500\n",
      "132/132 [==============================] - 0s 884us/step - loss: 1.3251 - accuracy: 0.5812 - val_loss: 1.0521 - val_accuracy: 0.6856\n",
      "Epoch 150/500\n",
      "132/132 [==============================] - 0s 901us/step - loss: 1.2925 - accuracy: 0.5721 - val_loss: 1.0493 - val_accuracy: 0.6833\n",
      "Epoch 151/500\n",
      "132/132 [==============================] - 0s 889us/step - loss: 1.2875 - accuracy: 0.5900 - val_loss: 1.0410 - val_accuracy: 0.6939\n",
      "Epoch 152/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2928 - accuracy: 0.5795 - val_loss: 1.0428 - val_accuracy: 0.6928\n",
      "Epoch 153/500\n",
      "132/132 [==============================] - 0s 851us/step - loss: 1.2887 - accuracy: 0.5790 - val_loss: 1.0419 - val_accuracy: 0.6867\n",
      "Epoch 154/500\n",
      "132/132 [==============================] - 0s 868us/step - loss: 1.2967 - accuracy: 0.5798 - val_loss: 1.0452 - val_accuracy: 0.6844\n",
      "Epoch 155/500\n",
      "132/132 [==============================] - 0s 902us/step - loss: 1.2978 - accuracy: 0.5790 - val_loss: 1.0410 - val_accuracy: 0.6972\n",
      "Epoch 156/500\n",
      "132/132 [==============================] - 0s 857us/step - loss: 1.3248 - accuracy: 0.5798 - val_loss: 1.0432 - val_accuracy: 0.6950\n",
      "Epoch 157/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.3308 - accuracy: 0.5702 - val_loss: 1.0461 - val_accuracy: 0.6917\n",
      "Epoch 158/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2834 - accuracy: 0.5698 - val_loss: 1.0412 - val_accuracy: 0.6889\n",
      "Epoch 159/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2937 - accuracy: 0.5860 - val_loss: 1.0399 - val_accuracy: 0.6928\n",
      "Epoch 160/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2867 - accuracy: 0.5864 - val_loss: 1.0391 - val_accuracy: 0.6944\n",
      "Epoch 161/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.3013 - accuracy: 0.5845 - val_loss: 1.0363 - val_accuracy: 0.6928\n",
      "Epoch 162/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.3057 - accuracy: 0.5826 - val_loss: 1.0391 - val_accuracy: 0.6872\n",
      "Epoch 163/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.3018 - accuracy: 0.5776 - val_loss: 1.0419 - val_accuracy: 0.6939\n",
      "Epoch 164/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.3093 - accuracy: 0.5781 - val_loss: 1.0387 - val_accuracy: 0.6867\n",
      "Epoch 165/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2822 - accuracy: 0.5883 - val_loss: 1.0380 - val_accuracy: 0.6906\n",
      "Epoch 166/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2800 - accuracy: 0.5898 - val_loss: 1.0356 - val_accuracy: 0.6944\n",
      "Epoch 167/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.3120 - accuracy: 0.5783 - val_loss: 1.0429 - val_accuracy: 0.6978\n",
      "Epoch 168/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2819 - accuracy: 0.5874 - val_loss: 1.0320 - val_accuracy: 0.6917\n",
      "Epoch 169/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.2921 - accuracy: 0.5907 - val_loss: 1.0393 - val_accuracy: 0.6889\n",
      "Epoch 170/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.2765 - accuracy: 0.5910 - val_loss: 1.0392 - val_accuracy: 0.6928\n",
      "Epoch 171/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2852 - accuracy: 0.5895 - val_loss: 1.0406 - val_accuracy: 0.6917\n",
      "Epoch 172/500\n",
      "132/132 [==============================] - 0s 864us/step - loss: 1.2908 - accuracy: 0.5814 - val_loss: 1.0410 - val_accuracy: 0.6939\n",
      "Epoch 173/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2978 - accuracy: 0.5824 - val_loss: 1.0459 - val_accuracy: 0.6861\n",
      "Epoch 174/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2892 - accuracy: 0.5938 - val_loss: 1.0525 - val_accuracy: 0.6867\n",
      "Epoch 175/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2814 - accuracy: 0.5879 - val_loss: 1.0461 - val_accuracy: 0.6867\n",
      "Epoch 176/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2985 - accuracy: 0.5805 - val_loss: 1.0447 - val_accuracy: 0.6911\n",
      "Epoch 177/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.3132 - accuracy: 0.5762 - val_loss: 1.0438 - val_accuracy: 0.6889\n",
      "Epoch 178/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.3218 - accuracy: 0.5798 - val_loss: 1.0556 - val_accuracy: 0.6861\n",
      "Epoch 179/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2897 - accuracy: 0.5900 - val_loss: 1.0500 - val_accuracy: 0.6867\n",
      "Epoch 180/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2892 - accuracy: 0.5860 - val_loss: 1.0444 - val_accuracy: 0.6889\n",
      "Epoch 181/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.3076 - accuracy: 0.5726 - val_loss: 1.0430 - val_accuracy: 0.6906\n",
      "Epoch 182/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2941 - accuracy: 0.5767 - val_loss: 1.0457 - val_accuracy: 0.6883\n",
      "Epoch 183/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2889 - accuracy: 0.5838 - val_loss: 1.0451 - val_accuracy: 0.6906\n",
      "Epoch 184/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2829 - accuracy: 0.5790 - val_loss: 1.0464 - val_accuracy: 0.6917\n",
      "Epoch 185/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2998 - accuracy: 0.5721 - val_loss: 1.0497 - val_accuracy: 0.6933\n",
      "Epoch 186/500\n",
      "132/132 [==============================] - 0s 852us/step - loss: 1.2966 - accuracy: 0.5783 - val_loss: 1.0431 - val_accuracy: 0.6911\n",
      "Epoch 187/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2820 - accuracy: 0.5855 - val_loss: 1.0433 - val_accuracy: 0.6906\n",
      "Epoch 188/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.3186 - accuracy: 0.5824 - val_loss: 1.0487 - val_accuracy: 0.6894\n",
      "Epoch 189/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2841 - accuracy: 0.5826 - val_loss: 1.0429 - val_accuracy: 0.6839\n",
      "Epoch 190/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2850 - accuracy: 0.5812 - val_loss: 1.0475 - val_accuracy: 0.6878\n",
      "Epoch 191/500\n",
      "132/132 [==============================] - 0s 851us/step - loss: 1.2861 - accuracy: 0.5857 - val_loss: 1.0451 - val_accuracy: 0.6883\n",
      "Epoch 192/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2928 - accuracy: 0.5776 - val_loss: 1.0488 - val_accuracy: 0.6878\n",
      "Epoch 193/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3077 - accuracy: 0.5700 - val_loss: 1.0462 - val_accuracy: 0.6933\n",
      "Epoch 194/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2908 - accuracy: 0.5729 - val_loss: 1.0453 - val_accuracy: 0.6911\n",
      "Epoch 195/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.3000 - accuracy: 0.5876 - val_loss: 1.0493 - val_accuracy: 0.6883\n",
      "Epoch 196/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.2898 - accuracy: 0.5752 - val_loss: 1.0460 - val_accuracy: 0.6878\n",
      "Epoch 197/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2768 - accuracy: 0.5919 - val_loss: 1.0446 - val_accuracy: 0.6922\n",
      "Epoch 198/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2769 - accuracy: 0.5814 - val_loss: 1.0474 - val_accuracy: 0.6917\n",
      "Epoch 199/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2876 - accuracy: 0.5893 - val_loss: 1.0500 - val_accuracy: 0.6894\n",
      "Epoch 200/500\n",
      "132/132 [==============================] - 0s 855us/step - loss: 1.2936 - accuracy: 0.5817 - val_loss: 1.0498 - val_accuracy: 0.6911\n",
      "Epoch 201/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.2928 - accuracy: 0.5774 - val_loss: 1.0504 - val_accuracy: 0.6917\n",
      "Epoch 202/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.2718 - accuracy: 0.5819 - val_loss: 1.0505 - val_accuracy: 0.6889\n",
      "Epoch 203/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.2904 - accuracy: 0.5814 - val_loss: 1.0467 - val_accuracy: 0.6944\n",
      "Epoch 204/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.3176 - accuracy: 0.5695 - val_loss: 1.0544 - val_accuracy: 0.6978\n",
      "Epoch 205/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.2797 - accuracy: 0.5833 - val_loss: 1.0464 - val_accuracy: 0.6906\n",
      "Epoch 206/500\n",
      "132/132 [==============================] - 0s 827us/step - loss: 1.2919 - accuracy: 0.5881 - val_loss: 1.0537 - val_accuracy: 0.6900\n",
      "Epoch 207/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2859 - accuracy: 0.5779 - val_loss: 1.0483 - val_accuracy: 0.6906\n",
      "Epoch 208/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.2787 - accuracy: 0.5771 - val_loss: 1.0457 - val_accuracy: 0.6861\n",
      "Epoch 209/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2937 - accuracy: 0.5807 - val_loss: 1.0496 - val_accuracy: 0.6872\n",
      "Epoch 210/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2878 - accuracy: 0.5821 - val_loss: 1.0460 - val_accuracy: 0.6917\n",
      "Epoch 211/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.3010 - accuracy: 0.5771 - val_loss: 1.0483 - val_accuracy: 0.6917\n",
      "Epoch 212/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.2846 - accuracy: 0.5838 - val_loss: 1.0481 - val_accuracy: 0.6956\n",
      "Epoch 213/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2679 - accuracy: 0.5852 - val_loss: 1.0495 - val_accuracy: 0.6900\n",
      "Epoch 214/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2985 - accuracy: 0.5814 - val_loss: 1.0495 - val_accuracy: 0.6872\n",
      "Epoch 215/500\n",
      "132/132 [==============================] - 0s 827us/step - loss: 1.2760 - accuracy: 0.5860 - val_loss: 1.0483 - val_accuracy: 0.6928\n",
      "Epoch 216/500\n",
      "132/132 [==============================] - 0s 852us/step - loss: 1.2922 - accuracy: 0.5879 - val_loss: 1.0475 - val_accuracy: 0.6922\n",
      "Epoch 217/500\n",
      "132/132 [==============================] - 0s 852us/step - loss: 1.2673 - accuracy: 0.5933 - val_loss: 1.0408 - val_accuracy: 0.6944\n",
      "Epoch 218/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2971 - accuracy: 0.5800 - val_loss: 1.0484 - val_accuracy: 0.6939\n",
      "Epoch 219/500\n",
      "132/132 [==============================] - 0s 832us/step - loss: 1.2855 - accuracy: 0.5836 - val_loss: 1.0433 - val_accuracy: 0.6889\n",
      "Epoch 220/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.3053 - accuracy: 0.5769 - val_loss: 1.0508 - val_accuracy: 0.6878\n",
      "Epoch 221/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.2910 - accuracy: 0.5829 - val_loss: 1.0520 - val_accuracy: 0.6900\n",
      "Epoch 222/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2668 - accuracy: 0.5771 - val_loss: 1.0421 - val_accuracy: 0.6894\n",
      "Epoch 223/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2887 - accuracy: 0.5745 - val_loss: 1.0445 - val_accuracy: 0.6894\n",
      "Epoch 224/500\n",
      "132/132 [==============================] - 0s 832us/step - loss: 1.2865 - accuracy: 0.5798 - val_loss: 1.0425 - val_accuracy: 0.6928\n",
      "Epoch 225/500\n",
      "132/132 [==============================] - 0s 866us/step - loss: 1.2828 - accuracy: 0.5879 - val_loss: 1.0410 - val_accuracy: 0.6939\n",
      "Epoch 226/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2797 - accuracy: 0.5764 - val_loss: 1.0418 - val_accuracy: 0.6928\n",
      "Epoch 227/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2455 - accuracy: 0.5869 - val_loss: 1.0437 - val_accuracy: 0.6889\n",
      "Epoch 228/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2818 - accuracy: 0.5886 - val_loss: 1.0560 - val_accuracy: 0.6867\n",
      "Epoch 229/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2701 - accuracy: 0.5879 - val_loss: 1.0382 - val_accuracy: 0.6961\n",
      "Epoch 230/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2735 - accuracy: 0.5895 - val_loss: 1.0414 - val_accuracy: 0.6917\n",
      "Epoch 231/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.3007 - accuracy: 0.5710 - val_loss: 1.0444 - val_accuracy: 0.6928\n",
      "Epoch 232/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2908 - accuracy: 0.5824 - val_loss: 1.0462 - val_accuracy: 0.6972\n",
      "Epoch 233/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2996 - accuracy: 0.5845 - val_loss: 1.0516 - val_accuracy: 0.6900\n",
      "Epoch 234/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2669 - accuracy: 0.5888 - val_loss: 1.0479 - val_accuracy: 0.6922\n",
      "Epoch 235/500\n",
      "132/132 [==============================] - 0s 857us/step - loss: 1.2719 - accuracy: 0.5810 - val_loss: 1.0440 - val_accuracy: 0.6861\n",
      "Epoch 236/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.3013 - accuracy: 0.5812 - val_loss: 1.0382 - val_accuracy: 0.6911\n",
      "Epoch 237/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.3030 - accuracy: 0.5760 - val_loss: 1.0439 - val_accuracy: 0.6939\n",
      "Epoch 238/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2889 - accuracy: 0.5755 - val_loss: 1.0471 - val_accuracy: 0.6900\n",
      "Epoch 239/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2649 - accuracy: 0.5793 - val_loss: 1.0477 - val_accuracy: 0.6900\n",
      "Epoch 240/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2878 - accuracy: 0.5824 - val_loss: 1.0453 - val_accuracy: 0.6922\n",
      "Epoch 241/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2842 - accuracy: 0.5760 - val_loss: 1.0445 - val_accuracy: 0.6922\n",
      "Epoch 242/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2543 - accuracy: 0.5876 - val_loss: 1.0415 - val_accuracy: 0.6922\n",
      "Epoch 243/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2830 - accuracy: 0.5855 - val_loss: 1.0383 - val_accuracy: 0.6939\n",
      "Epoch 244/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2857 - accuracy: 0.5807 - val_loss: 1.0430 - val_accuracy: 0.6911\n",
      "Epoch 245/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2905 - accuracy: 0.5881 - val_loss: 1.0442 - val_accuracy: 0.6889\n",
      "Epoch 246/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2672 - accuracy: 0.5857 - val_loss: 1.0415 - val_accuracy: 0.6878\n",
      "Epoch 247/500\n",
      "132/132 [==============================] - 0s 858us/step - loss: 1.2679 - accuracy: 0.5979 - val_loss: 1.0390 - val_accuracy: 0.6900\n",
      "Epoch 248/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2906 - accuracy: 0.5805 - val_loss: 1.0486 - val_accuracy: 0.6900\n",
      "Epoch 249/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2805 - accuracy: 0.5807 - val_loss: 1.0456 - val_accuracy: 0.6978\n",
      "Epoch 250/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2816 - accuracy: 0.5824 - val_loss: 1.0457 - val_accuracy: 0.6944\n",
      "Epoch 251/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.2839 - accuracy: 0.5824 - val_loss: 1.0416 - val_accuracy: 0.6928\n",
      "Epoch 252/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2808 - accuracy: 0.5855 - val_loss: 1.0466 - val_accuracy: 0.6950\n",
      "Epoch 253/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2975 - accuracy: 0.5745 - val_loss: 1.0424 - val_accuracy: 0.6944\n",
      "Epoch 254/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.2760 - accuracy: 0.5795 - val_loss: 1.0441 - val_accuracy: 0.6894\n",
      "Epoch 255/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2960 - accuracy: 0.5755 - val_loss: 1.0452 - val_accuracy: 0.6911\n",
      "Epoch 256/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2901 - accuracy: 0.5810 - val_loss: 1.0501 - val_accuracy: 0.6944\n",
      "Epoch 257/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2807 - accuracy: 0.5757 - val_loss: 1.0481 - val_accuracy: 0.6889\n",
      "Epoch 258/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2788 - accuracy: 0.5807 - val_loss: 1.0486 - val_accuracy: 0.6972\n",
      "Epoch 259/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2848 - accuracy: 0.5860 - val_loss: 1.0484 - val_accuracy: 0.6906\n",
      "Epoch 260/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.2838 - accuracy: 0.5862 - val_loss: 1.0451 - val_accuracy: 0.6928\n",
      "Epoch 261/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2710 - accuracy: 0.5862 - val_loss: 1.0381 - val_accuracy: 0.6956\n",
      "Epoch 262/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2563 - accuracy: 0.5964 - val_loss: 1.0392 - val_accuracy: 0.6928\n",
      "Epoch 263/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2621 - accuracy: 0.5860 - val_loss: 1.0352 - val_accuracy: 0.6967\n",
      "Epoch 264/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2822 - accuracy: 0.5798 - val_loss: 1.0419 - val_accuracy: 0.6944\n",
      "Epoch 265/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2898 - accuracy: 0.5852 - val_loss: 1.0446 - val_accuracy: 0.6944\n",
      "Epoch 266/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2558 - accuracy: 0.5962 - val_loss: 1.0482 - val_accuracy: 0.6922\n",
      "Epoch 267/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2780 - accuracy: 0.5881 - val_loss: 1.0421 - val_accuracy: 0.6928\n",
      "Epoch 268/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2785 - accuracy: 0.5898 - val_loss: 1.0491 - val_accuracy: 0.6906\n",
      "Epoch 269/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2691 - accuracy: 0.5945 - val_loss: 1.0473 - val_accuracy: 0.6883\n",
      "Epoch 270/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2935 - accuracy: 0.5776 - val_loss: 1.0430 - val_accuracy: 0.6922\n",
      "Epoch 271/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2759 - accuracy: 0.5893 - val_loss: 1.0490 - val_accuracy: 0.6944\n",
      "Epoch 272/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2830 - accuracy: 0.5807 - val_loss: 1.0413 - val_accuracy: 0.6906\n",
      "Epoch 273/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2882 - accuracy: 0.5831 - val_loss: 1.0452 - val_accuracy: 0.6911\n",
      "Epoch 274/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2625 - accuracy: 0.5890 - val_loss: 1.0464 - val_accuracy: 0.6878\n",
      "Epoch 275/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2805 - accuracy: 0.5717 - val_loss: 1.0518 - val_accuracy: 0.6867\n",
      "Epoch 276/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2752 - accuracy: 0.5840 - val_loss: 1.0528 - val_accuracy: 0.6817\n",
      "Epoch 277/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2599 - accuracy: 0.5879 - val_loss: 1.0529 - val_accuracy: 0.6800\n",
      "Epoch 278/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2713 - accuracy: 0.5845 - val_loss: 1.0503 - val_accuracy: 0.6872\n",
      "Epoch 279/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2732 - accuracy: 0.5924 - val_loss: 1.0465 - val_accuracy: 0.6911\n",
      "Epoch 280/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2970 - accuracy: 0.5843 - val_loss: 1.0484 - val_accuracy: 0.6900\n",
      "Epoch 281/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.2725 - accuracy: 0.5905 - val_loss: 1.0504 - val_accuracy: 0.6878\n",
      "Epoch 282/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2839 - accuracy: 0.5929 - val_loss: 1.0489 - val_accuracy: 0.6911\n",
      "Epoch 283/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2643 - accuracy: 0.5879 - val_loss: 1.0527 - val_accuracy: 0.6933\n",
      "Epoch 284/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2754 - accuracy: 0.5907 - val_loss: 1.0516 - val_accuracy: 0.6961\n",
      "Epoch 285/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2812 - accuracy: 0.5860 - val_loss: 1.0529 - val_accuracy: 0.6911\n",
      "Epoch 286/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2749 - accuracy: 0.5848 - val_loss: 1.0444 - val_accuracy: 0.6939\n",
      "Epoch 287/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2662 - accuracy: 0.5940 - val_loss: 1.0511 - val_accuracy: 0.6861\n",
      "Epoch 288/500\n",
      "132/132 [==============================] - 0s 862us/step - loss: 1.2641 - accuracy: 0.5948 - val_loss: 1.0522 - val_accuracy: 0.6939\n",
      "Epoch 289/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2864 - accuracy: 0.5824 - val_loss: 1.0517 - val_accuracy: 0.6917\n",
      "Epoch 290/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2752 - accuracy: 0.5886 - val_loss: 1.0506 - val_accuracy: 0.6856\n",
      "Epoch 291/500\n",
      "132/132 [==============================] - 0s 853us/step - loss: 1.2958 - accuracy: 0.5781 - val_loss: 1.0473 - val_accuracy: 0.6917\n",
      "Epoch 292/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2736 - accuracy: 0.5862 - val_loss: 1.0468 - val_accuracy: 0.6906\n",
      "Epoch 293/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2751 - accuracy: 0.5902 - val_loss: 1.0441 - val_accuracy: 0.6922\n",
      "Epoch 294/500\n",
      "132/132 [==============================] - 0s 854us/step - loss: 1.2884 - accuracy: 0.5767 - val_loss: 1.0442 - val_accuracy: 0.6889\n",
      "Epoch 295/500\n",
      "132/132 [==============================] - 0s 848us/step - loss: 1.2830 - accuracy: 0.5871 - val_loss: 1.0509 - val_accuracy: 0.6900\n",
      "Epoch 296/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2810 - accuracy: 0.5814 - val_loss: 1.0461 - val_accuracy: 0.6917\n",
      "Epoch 297/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2857 - accuracy: 0.5860 - val_loss: 1.0465 - val_accuracy: 0.6950\n",
      "Epoch 298/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.3018 - accuracy: 0.5881 - val_loss: 1.0481 - val_accuracy: 0.6883\n",
      "Epoch 299/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2810 - accuracy: 0.5826 - val_loss: 1.0436 - val_accuracy: 0.6961\n",
      "Epoch 300/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2720 - accuracy: 0.5912 - val_loss: 1.0434 - val_accuracy: 0.6939\n",
      "Epoch 301/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.2825 - accuracy: 0.5864 - val_loss: 1.0485 - val_accuracy: 0.6933\n",
      "Epoch 302/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2737 - accuracy: 0.5860 - val_loss: 1.0441 - val_accuracy: 0.6961\n",
      "Epoch 303/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2583 - accuracy: 0.5929 - val_loss: 1.0432 - val_accuracy: 0.7011\n",
      "Epoch 304/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2784 - accuracy: 0.5924 - val_loss: 1.0441 - val_accuracy: 0.6994\n",
      "Epoch 305/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2967 - accuracy: 0.5810 - val_loss: 1.0428 - val_accuracy: 0.6917\n",
      "Epoch 306/500\n",
      "132/132 [==============================] - 0s 870us/step - loss: 1.2873 - accuracy: 0.5779 - val_loss: 1.0426 - val_accuracy: 0.6972\n",
      "Epoch 307/500\n",
      "132/132 [==============================] - 0s 848us/step - loss: 1.2836 - accuracy: 0.5821 - val_loss: 1.0408 - val_accuracy: 0.6950\n",
      "Epoch 308/500\n",
      "132/132 [==============================] - 0s 848us/step - loss: 1.2708 - accuracy: 0.5874 - val_loss: 1.0415 - val_accuracy: 0.6950\n",
      "Epoch 309/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2670 - accuracy: 0.5840 - val_loss: 1.0366 - val_accuracy: 0.6972\n",
      "Epoch 310/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2898 - accuracy: 0.5902 - val_loss: 1.0380 - val_accuracy: 0.6972\n",
      "Epoch 311/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2903 - accuracy: 0.5883 - val_loss: 1.0458 - val_accuracy: 0.6978\n",
      "Epoch 312/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2860 - accuracy: 0.5852 - val_loss: 1.0430 - val_accuracy: 0.6956\n",
      "Epoch 313/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2946 - accuracy: 0.5883 - val_loss: 1.0428 - val_accuracy: 0.6950\n",
      "Epoch 314/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2718 - accuracy: 0.5786 - val_loss: 1.0421 - val_accuracy: 0.6956\n",
      "Epoch 315/500\n",
      "132/132 [==============================] - 0s 848us/step - loss: 1.2644 - accuracy: 0.5900 - val_loss: 1.0404 - val_accuracy: 0.6911\n",
      "Epoch 316/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2801 - accuracy: 0.5805 - val_loss: 1.0451 - val_accuracy: 0.6906\n",
      "Epoch 317/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2748 - accuracy: 0.5852 - val_loss: 1.0467 - val_accuracy: 0.6883\n",
      "Epoch 318/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2622 - accuracy: 0.5945 - val_loss: 1.0444 - val_accuracy: 0.6894\n",
      "Epoch 319/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2922 - accuracy: 0.5893 - val_loss: 1.0480 - val_accuracy: 0.6850\n",
      "Epoch 320/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2680 - accuracy: 0.5743 - val_loss: 1.0531 - val_accuracy: 0.6861\n",
      "Epoch 321/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2939 - accuracy: 0.5900 - val_loss: 1.0549 - val_accuracy: 0.6822\n",
      "Epoch 322/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2880 - accuracy: 0.5848 - val_loss: 1.0584 - val_accuracy: 0.6856\n",
      "Epoch 323/500\n",
      "132/132 [==============================] - 0s 877us/step - loss: 1.2461 - accuracy: 0.5857 - val_loss: 1.0527 - val_accuracy: 0.6822\n",
      "Epoch 324/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2693 - accuracy: 0.5807 - val_loss: 1.0536 - val_accuracy: 0.6822\n",
      "Epoch 325/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2898 - accuracy: 0.5907 - val_loss: 1.0536 - val_accuracy: 0.6900\n",
      "Epoch 326/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2617 - accuracy: 0.5912 - val_loss: 1.0520 - val_accuracy: 0.6906\n",
      "Epoch 327/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.3011 - accuracy: 0.5819 - val_loss: 1.0594 - val_accuracy: 0.6878\n",
      "Epoch 328/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2569 - accuracy: 0.5776 - val_loss: 1.0603 - val_accuracy: 0.6822\n",
      "Epoch 329/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2644 - accuracy: 0.5905 - val_loss: 1.0503 - val_accuracy: 0.6922\n",
      "Epoch 330/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2858 - accuracy: 0.5852 - val_loss: 1.0505 - val_accuracy: 0.6844\n",
      "Epoch 331/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2762 - accuracy: 0.5874 - val_loss: 1.0464 - val_accuracy: 0.6872\n",
      "Epoch 332/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2651 - accuracy: 0.5988 - val_loss: 1.0512 - val_accuracy: 0.6844\n",
      "Epoch 333/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2706 - accuracy: 0.5886 - val_loss: 1.0477 - val_accuracy: 0.6878\n",
      "Epoch 334/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2756 - accuracy: 0.5940 - val_loss: 1.0482 - val_accuracy: 0.6883\n",
      "Epoch 335/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2863 - accuracy: 0.5764 - val_loss: 1.0530 - val_accuracy: 0.6844\n",
      "Epoch 336/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2944 - accuracy: 0.5848 - val_loss: 1.0558 - val_accuracy: 0.6872\n",
      "Epoch 337/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2870 - accuracy: 0.5717 - val_loss: 1.0527 - val_accuracy: 0.6867\n",
      "Epoch 338/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2618 - accuracy: 0.5867 - val_loss: 1.0472 - val_accuracy: 0.6850\n",
      "Epoch 339/500\n",
      "132/132 [==============================] - 0s 848us/step - loss: 1.2612 - accuracy: 0.5962 - val_loss: 1.0448 - val_accuracy: 0.6911\n",
      "Epoch 340/500\n",
      "132/132 [==============================] - 0s 851us/step - loss: 1.2979 - accuracy: 0.5798 - val_loss: 1.0446 - val_accuracy: 0.6867\n",
      "Epoch 341/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2745 - accuracy: 0.5938 - val_loss: 1.0466 - val_accuracy: 0.6878\n",
      "Epoch 342/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2933 - accuracy: 0.5800 - val_loss: 1.0545 - val_accuracy: 0.6856\n",
      "Epoch 343/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2752 - accuracy: 0.5838 - val_loss: 1.0484 - val_accuracy: 0.6900\n",
      "Epoch 344/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2785 - accuracy: 0.5819 - val_loss: 1.0540 - val_accuracy: 0.6900\n",
      "Epoch 345/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2712 - accuracy: 0.5879 - val_loss: 1.0617 - val_accuracy: 0.6900\n",
      "Epoch 346/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.2752 - accuracy: 0.5890 - val_loss: 1.0581 - val_accuracy: 0.6900\n",
      "Epoch 347/500\n",
      "132/132 [==============================] - 0s 832us/step - loss: 1.2700 - accuracy: 0.5807 - val_loss: 1.0486 - val_accuracy: 0.6872\n",
      "Epoch 348/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2654 - accuracy: 0.5881 - val_loss: 1.0458 - val_accuracy: 0.6911\n",
      "Epoch 349/500\n",
      "132/132 [==============================] - 0s 832us/step - loss: 1.2744 - accuracy: 0.5843 - val_loss: 1.0518 - val_accuracy: 0.6883\n",
      "Epoch 350/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2730 - accuracy: 0.5833 - val_loss: 1.0536 - val_accuracy: 0.6839\n",
      "Epoch 351/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2697 - accuracy: 0.5895 - val_loss: 1.0559 - val_accuracy: 0.6850\n",
      "Epoch 352/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2697 - accuracy: 0.5850 - val_loss: 1.0569 - val_accuracy: 0.6856\n",
      "Epoch 353/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2771 - accuracy: 0.5860 - val_loss: 1.0629 - val_accuracy: 0.6822\n",
      "Epoch 354/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2705 - accuracy: 0.5812 - val_loss: 1.0577 - val_accuracy: 0.6811\n",
      "Epoch 355/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2580 - accuracy: 0.5933 - val_loss: 1.0493 - val_accuracy: 0.6894\n",
      "Epoch 356/500\n",
      "132/132 [==============================] - 0s 852us/step - loss: 1.2918 - accuracy: 0.5883 - val_loss: 1.0543 - val_accuracy: 0.6878\n",
      "Epoch 357/500\n",
      "132/132 [==============================] - 0s 865us/step - loss: 1.2869 - accuracy: 0.5855 - val_loss: 1.0549 - val_accuracy: 0.6850\n",
      "Epoch 358/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2753 - accuracy: 0.5831 - val_loss: 1.0451 - val_accuracy: 0.6911\n",
      "Epoch 359/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2719 - accuracy: 0.5952 - val_loss: 1.0484 - val_accuracy: 0.6861\n",
      "Epoch 360/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2888 - accuracy: 0.5838 - val_loss: 1.0479 - val_accuracy: 0.6878\n",
      "Epoch 361/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2754 - accuracy: 0.5850 - val_loss: 1.0479 - val_accuracy: 0.6911\n",
      "Epoch 362/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2886 - accuracy: 0.5821 - val_loss: 1.0433 - val_accuracy: 0.6922\n",
      "Epoch 363/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2636 - accuracy: 0.5874 - val_loss: 1.0470 - val_accuracy: 0.6917\n",
      "Epoch 364/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2841 - accuracy: 0.5948 - val_loss: 1.0457 - val_accuracy: 0.6933\n",
      "Epoch 365/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2865 - accuracy: 0.5907 - val_loss: 1.0445 - val_accuracy: 0.6900\n",
      "Epoch 366/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2820 - accuracy: 0.5781 - val_loss: 1.0482 - val_accuracy: 0.6878\n",
      "Epoch 367/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2775 - accuracy: 0.5795 - val_loss: 1.0505 - val_accuracy: 0.6839\n",
      "Epoch 368/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2794 - accuracy: 0.5807 - val_loss: 1.0462 - val_accuracy: 0.6872\n",
      "Epoch 369/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2920 - accuracy: 0.5829 - val_loss: 1.0482 - val_accuracy: 0.6900\n",
      "Epoch 370/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2795 - accuracy: 0.5862 - val_loss: 1.0494 - val_accuracy: 0.6878\n",
      "Epoch 371/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2825 - accuracy: 0.5857 - val_loss: 1.0488 - val_accuracy: 0.6906\n",
      "Epoch 372/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2599 - accuracy: 0.5879 - val_loss: 1.0533 - val_accuracy: 0.6833\n",
      "Epoch 373/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.2919 - accuracy: 0.5817 - val_loss: 1.0554 - val_accuracy: 0.6856\n",
      "Epoch 374/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.2759 - accuracy: 0.5883 - val_loss: 1.0466 - val_accuracy: 0.6839\n",
      "Epoch 375/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2597 - accuracy: 0.5840 - val_loss: 1.0457 - val_accuracy: 0.6900\n",
      "Epoch 376/500\n",
      "132/132 [==============================] - 0s 824us/step - loss: 1.2379 - accuracy: 0.5993 - val_loss: 1.0525 - val_accuracy: 0.6856\n",
      "Epoch 377/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2740 - accuracy: 0.5921 - val_loss: 1.0513 - val_accuracy: 0.6883\n",
      "Epoch 378/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2796 - accuracy: 0.5862 - val_loss: 1.0475 - val_accuracy: 0.6878\n",
      "Epoch 379/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2484 - accuracy: 0.5931 - val_loss: 1.0413 - val_accuracy: 0.6928\n",
      "Epoch 380/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2788 - accuracy: 0.5869 - val_loss: 1.0443 - val_accuracy: 0.6928\n",
      "Epoch 381/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2812 - accuracy: 0.5952 - val_loss: 1.0449 - val_accuracy: 0.6900\n",
      "Epoch 382/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2835 - accuracy: 0.5867 - val_loss: 1.0518 - val_accuracy: 0.6817\n",
      "Epoch 383/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2796 - accuracy: 0.5783 - val_loss: 1.0477 - val_accuracy: 0.6917\n",
      "Epoch 384/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2984 - accuracy: 0.5900 - val_loss: 1.0447 - val_accuracy: 0.6917\n",
      "Epoch 385/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.2704 - accuracy: 0.5812 - val_loss: 1.0437 - val_accuracy: 0.6922\n",
      "Epoch 386/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2755 - accuracy: 0.5907 - val_loss: 1.0451 - val_accuracy: 0.6900\n",
      "Epoch 387/500\n",
      "132/132 [==============================] - 0s 863us/step - loss: 1.2755 - accuracy: 0.5874 - val_loss: 1.0471 - val_accuracy: 0.6911\n",
      "Epoch 388/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2870 - accuracy: 0.5914 - val_loss: 1.0465 - val_accuracy: 0.6939\n",
      "Epoch 389/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2501 - accuracy: 0.5917 - val_loss: 1.0499 - val_accuracy: 0.6850\n",
      "Epoch 390/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2630 - accuracy: 0.5819 - val_loss: 1.0477 - val_accuracy: 0.6878\n",
      "Epoch 391/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2666 - accuracy: 0.5924 - val_loss: 1.0470 - val_accuracy: 0.6939\n",
      "Epoch 392/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2780 - accuracy: 0.5831 - val_loss: 1.0470 - val_accuracy: 0.6889\n",
      "Epoch 393/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2820 - accuracy: 0.5871 - val_loss: 1.0500 - val_accuracy: 0.6867\n",
      "Epoch 394/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2778 - accuracy: 0.5805 - val_loss: 1.0501 - val_accuracy: 0.6883\n",
      "Epoch 395/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2936 - accuracy: 0.5764 - val_loss: 1.0509 - val_accuracy: 0.6894\n",
      "Epoch 396/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2643 - accuracy: 0.5895 - val_loss: 1.0510 - val_accuracy: 0.6894\n",
      "Epoch 397/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2705 - accuracy: 0.5931 - val_loss: 1.0501 - val_accuracy: 0.6867\n",
      "Epoch 398/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2524 - accuracy: 0.5852 - val_loss: 1.0528 - val_accuracy: 0.6872\n",
      "Epoch 399/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2713 - accuracy: 0.5890 - val_loss: 1.0495 - val_accuracy: 0.6894\n",
      "Epoch 400/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2713 - accuracy: 0.5876 - val_loss: 1.0550 - val_accuracy: 0.6856\n",
      "Epoch 401/500\n",
      "132/132 [==============================] - 0s 868us/step - loss: 1.2681 - accuracy: 0.5840 - val_loss: 1.0510 - val_accuracy: 0.6961\n",
      "Epoch 402/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2737 - accuracy: 0.5755 - val_loss: 1.0550 - val_accuracy: 0.6844\n",
      "Epoch 403/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2799 - accuracy: 0.5862 - val_loss: 1.0578 - val_accuracy: 0.6900\n",
      "Epoch 404/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2842 - accuracy: 0.5895 - val_loss: 1.0555 - val_accuracy: 0.6939\n",
      "Epoch 405/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2731 - accuracy: 0.5805 - val_loss: 1.0547 - val_accuracy: 0.6906\n",
      "Epoch 406/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2674 - accuracy: 0.5886 - val_loss: 1.0537 - val_accuracy: 0.6878\n",
      "Epoch 407/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2676 - accuracy: 0.5895 - val_loss: 1.0554 - val_accuracy: 0.6906\n",
      "Epoch 408/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2762 - accuracy: 0.5924 - val_loss: 1.0532 - val_accuracy: 0.6889\n",
      "Epoch 409/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2720 - accuracy: 0.5926 - val_loss: 1.0539 - val_accuracy: 0.6828\n",
      "Epoch 410/500\n",
      "132/132 [==============================] - 0s 875us/step - loss: 1.2678 - accuracy: 0.5848 - val_loss: 1.0451 - val_accuracy: 0.6833\n",
      "Epoch 411/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2753 - accuracy: 0.5898 - val_loss: 1.0499 - val_accuracy: 0.6928\n",
      "Epoch 412/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2601 - accuracy: 0.5957 - val_loss: 1.0498 - val_accuracy: 0.6844\n",
      "Epoch 413/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2707 - accuracy: 0.5952 - val_loss: 1.0538 - val_accuracy: 0.6783\n",
      "Epoch 414/500\n",
      "132/132 [==============================] - 0s 875us/step - loss: 1.2677 - accuracy: 0.5926 - val_loss: 1.0550 - val_accuracy: 0.6878\n",
      "Epoch 415/500\n",
      "132/132 [==============================] - 0s 857us/step - loss: 1.2864 - accuracy: 0.5860 - val_loss: 1.0570 - val_accuracy: 0.6828\n",
      "Epoch 416/500\n",
      "132/132 [==============================] - 0s 852us/step - loss: 1.2814 - accuracy: 0.5764 - val_loss: 1.0615 - val_accuracy: 0.6789\n",
      "Epoch 417/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2702 - accuracy: 0.5812 - val_loss: 1.0587 - val_accuracy: 0.6889\n",
      "Epoch 418/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2999 - accuracy: 0.5857 - val_loss: 1.0608 - val_accuracy: 0.6906\n",
      "Epoch 419/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2434 - accuracy: 0.5938 - val_loss: 1.0534 - val_accuracy: 0.6883\n",
      "Epoch 420/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2696 - accuracy: 0.5893 - val_loss: 1.0513 - val_accuracy: 0.6917\n",
      "Epoch 421/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2568 - accuracy: 0.5933 - val_loss: 1.0554 - val_accuracy: 0.6894\n",
      "Epoch 422/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2493 - accuracy: 0.5943 - val_loss: 1.0618 - val_accuracy: 0.6883\n",
      "Epoch 423/500\n",
      "132/132 [==============================] - 0s 848us/step - loss: 1.2668 - accuracy: 0.5862 - val_loss: 1.0681 - val_accuracy: 0.6828\n",
      "Epoch 424/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2663 - accuracy: 0.5962 - val_loss: 1.0642 - val_accuracy: 0.6844\n",
      "Epoch 425/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2869 - accuracy: 0.5874 - val_loss: 1.0614 - val_accuracy: 0.6867\n",
      "Epoch 426/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2961 - accuracy: 0.5802 - val_loss: 1.0617 - val_accuracy: 0.6817\n",
      "Epoch 427/500\n",
      "132/132 [==============================] - 0s 858us/step - loss: 1.2378 - accuracy: 0.6064 - val_loss: 1.0526 - val_accuracy: 0.6789\n",
      "Epoch 428/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2708 - accuracy: 0.5917 - val_loss: 1.0573 - val_accuracy: 0.6839\n",
      "Epoch 429/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2613 - accuracy: 0.5960 - val_loss: 1.0605 - val_accuracy: 0.6800\n",
      "Epoch 430/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2883 - accuracy: 0.5812 - val_loss: 1.0671 - val_accuracy: 0.6717\n",
      "Epoch 431/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2560 - accuracy: 0.5950 - val_loss: 1.0583 - val_accuracy: 0.6800\n",
      "Epoch 432/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2627 - accuracy: 0.5838 - val_loss: 1.0548 - val_accuracy: 0.6794\n",
      "Epoch 433/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2649 - accuracy: 0.5871 - val_loss: 1.0537 - val_accuracy: 0.6856\n",
      "Epoch 434/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2573 - accuracy: 0.5986 - val_loss: 1.0524 - val_accuracy: 0.6844\n",
      "Epoch 435/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2736 - accuracy: 0.5893 - val_loss: 1.0522 - val_accuracy: 0.6850\n",
      "Epoch 436/500\n",
      "132/132 [==============================] - 0s 867us/step - loss: 1.2670 - accuracy: 0.5814 - val_loss: 1.0576 - val_accuracy: 0.6817\n",
      "Epoch 437/500\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.2657 - accuracy: 0.5881 - val_loss: 1.0594 - val_accuracy: 0.6856\n",
      "Epoch 438/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2624 - accuracy: 0.5900 - val_loss: 1.0546 - val_accuracy: 0.6861\n",
      "Epoch 439/500\n",
      "132/132 [==============================] - 0s 872us/step - loss: 1.2801 - accuracy: 0.5812 - val_loss: 1.0520 - val_accuracy: 0.6972\n",
      "Epoch 440/500\n",
      "132/132 [==============================] - 0s 836us/step - loss: 1.2825 - accuracy: 0.5793 - val_loss: 1.0519 - val_accuracy: 0.6939\n",
      "Epoch 441/500\n",
      "132/132 [==============================] - 0s 828us/step - loss: 1.2655 - accuracy: 0.5829 - val_loss: 1.0526 - val_accuracy: 0.6922\n",
      "Epoch 442/500\n",
      "132/132 [==============================] - 0s 846us/step - loss: 1.2554 - accuracy: 0.5843 - val_loss: 1.0478 - val_accuracy: 0.6939\n",
      "Epoch 443/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2908 - accuracy: 0.5845 - val_loss: 1.0508 - val_accuracy: 0.6922\n",
      "Epoch 444/500\n",
      "132/132 [==============================] - 0s 829us/step - loss: 1.2804 - accuracy: 0.5852 - val_loss: 1.0518 - val_accuracy: 0.6872\n",
      "Epoch 445/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2596 - accuracy: 0.5969 - val_loss: 1.0528 - val_accuracy: 0.6911\n",
      "Epoch 446/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2578 - accuracy: 0.5921 - val_loss: 1.0497 - val_accuracy: 0.6889\n",
      "Epoch 447/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2839 - accuracy: 0.5910 - val_loss: 1.0470 - val_accuracy: 0.6928\n",
      "Epoch 448/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2699 - accuracy: 0.5850 - val_loss: 1.0523 - val_accuracy: 0.6906\n",
      "Epoch 449/500\n",
      "132/132 [==============================] - 0s 826us/step - loss: 1.2584 - accuracy: 0.5914 - val_loss: 1.0516 - val_accuracy: 0.6883\n",
      "Epoch 450/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2568 - accuracy: 0.5936 - val_loss: 1.0461 - val_accuracy: 0.6922\n",
      "Epoch 451/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2967 - accuracy: 0.5948 - val_loss: 1.0522 - val_accuracy: 0.6933\n",
      "Epoch 452/500\n",
      "132/132 [==============================] - 0s 863us/step - loss: 1.2647 - accuracy: 0.5838 - val_loss: 1.0506 - val_accuracy: 0.6967\n",
      "Epoch 453/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2645 - accuracy: 0.5926 - val_loss: 1.0529 - val_accuracy: 0.6894\n",
      "Epoch 454/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2904 - accuracy: 0.5833 - val_loss: 1.0562 - val_accuracy: 0.6900\n",
      "Epoch 455/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2827 - accuracy: 0.5898 - val_loss: 1.0571 - val_accuracy: 0.6894\n",
      "Epoch 456/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2625 - accuracy: 0.5931 - val_loss: 1.0529 - val_accuracy: 0.6861\n",
      "Epoch 457/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2907 - accuracy: 0.5855 - val_loss: 1.0556 - val_accuracy: 0.6944\n",
      "Epoch 458/500\n",
      "132/132 [==============================] - 0s 848us/step - loss: 1.2602 - accuracy: 0.5864 - val_loss: 1.0518 - val_accuracy: 0.6889\n",
      "Epoch 459/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2606 - accuracy: 0.5905 - val_loss: 1.0530 - val_accuracy: 0.6911\n",
      "Epoch 460/500\n",
      "132/132 [==============================] - 0s 908us/step - loss: 1.2649 - accuracy: 0.5817 - val_loss: 1.0521 - val_accuracy: 0.6889\n",
      "Epoch 461/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2699 - accuracy: 0.5845 - val_loss: 1.0518 - val_accuracy: 0.6844\n",
      "Epoch 462/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2544 - accuracy: 0.5926 - val_loss: 1.0505 - val_accuracy: 0.6900\n",
      "Epoch 463/500\n",
      "132/132 [==============================] - 0s 834us/step - loss: 1.2850 - accuracy: 0.5871 - val_loss: 1.0515 - val_accuracy: 0.6922\n",
      "Epoch 464/500\n",
      "132/132 [==============================] - 0s 859us/step - loss: 1.2650 - accuracy: 0.5981 - val_loss: 1.0513 - val_accuracy: 0.6928\n",
      "Epoch 465/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2593 - accuracy: 0.5933 - val_loss: 1.0546 - val_accuracy: 0.6878\n",
      "Epoch 466/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2964 - accuracy: 0.5857 - val_loss: 1.0591 - val_accuracy: 0.6867\n",
      "Epoch 467/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2774 - accuracy: 0.5829 - val_loss: 1.0549 - val_accuracy: 0.6822\n",
      "Epoch 468/500\n",
      "132/132 [==============================] - 0s 830us/step - loss: 1.2602 - accuracy: 0.6017 - val_loss: 1.0559 - val_accuracy: 0.6850\n",
      "Epoch 469/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2545 - accuracy: 0.5929 - val_loss: 1.0466 - val_accuracy: 0.6917\n",
      "Epoch 470/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2984 - accuracy: 0.5807 - val_loss: 1.0495 - val_accuracy: 0.6872\n",
      "Epoch 471/500\n",
      "132/132 [==============================] - 0s 835us/step - loss: 1.2685 - accuracy: 0.6012 - val_loss: 1.0559 - val_accuracy: 0.6856\n",
      "Epoch 472/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2682 - accuracy: 0.5933 - val_loss: 1.0435 - val_accuracy: 0.6889\n",
      "Epoch 473/500\n",
      "132/132 [==============================] - 0s 851us/step - loss: 1.2634 - accuracy: 0.5933 - val_loss: 1.0480 - val_accuracy: 0.6961\n",
      "Epoch 474/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2562 - accuracy: 0.5964 - val_loss: 1.0511 - val_accuracy: 0.6844\n",
      "Epoch 475/500\n",
      "132/132 [==============================] - 0s 838us/step - loss: 1.2524 - accuracy: 0.5919 - val_loss: 1.0512 - val_accuracy: 0.6872\n",
      "Epoch 476/500\n",
      "132/132 [==============================] - 0s 873us/step - loss: 1.2505 - accuracy: 0.6043 - val_loss: 1.0594 - val_accuracy: 0.6822\n",
      "Epoch 477/500\n",
      "132/132 [==============================] - 0s 850us/step - loss: 1.2664 - accuracy: 0.5862 - val_loss: 1.0570 - val_accuracy: 0.6894\n",
      "Epoch 478/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2717 - accuracy: 0.5893 - val_loss: 1.0571 - val_accuracy: 0.6878\n",
      "Epoch 479/500\n",
      "132/132 [==============================] - 0s 841us/step - loss: 1.2771 - accuracy: 0.5917 - val_loss: 1.0506 - val_accuracy: 0.6906\n",
      "Epoch 480/500\n",
      "132/132 [==============================] - 0s 833us/step - loss: 1.2549 - accuracy: 0.5969 - val_loss: 1.0509 - val_accuracy: 0.6872\n",
      "Epoch 481/500\n",
      "132/132 [==============================] - 0s 849us/step - loss: 1.2677 - accuracy: 0.5945 - val_loss: 1.0501 - val_accuracy: 0.6922\n",
      "Epoch 482/500\n",
      "132/132 [==============================] - 0s 843us/step - loss: 1.2556 - accuracy: 0.6010 - val_loss: 1.0515 - val_accuracy: 0.6933\n",
      "Epoch 483/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2806 - accuracy: 0.5831 - val_loss: 1.0558 - val_accuracy: 0.6950\n",
      "Epoch 484/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2772 - accuracy: 0.5879 - val_loss: 1.0538 - val_accuracy: 0.6928\n",
      "Epoch 485/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2753 - accuracy: 0.5869 - val_loss: 1.0571 - val_accuracy: 0.6944\n",
      "Epoch 486/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2546 - accuracy: 0.6012 - val_loss: 1.0531 - val_accuracy: 0.6872\n",
      "Epoch 487/500\n",
      "132/132 [==============================] - 0s 856us/step - loss: 1.2639 - accuracy: 0.5912 - val_loss: 1.0536 - val_accuracy: 0.6867\n",
      "Epoch 488/500\n",
      "132/132 [==============================] - 0s 832us/step - loss: 1.2545 - accuracy: 0.5950 - val_loss: 1.0541 - val_accuracy: 0.6833\n",
      "Epoch 489/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2722 - accuracy: 0.5867 - val_loss: 1.0508 - val_accuracy: 0.6900\n",
      "Epoch 490/500\n",
      "132/132 [==============================] - 0s 831us/step - loss: 1.2564 - accuracy: 0.5974 - val_loss: 1.0523 - val_accuracy: 0.6894\n",
      "Epoch 491/500\n",
      "132/132 [==============================] - 0s 837us/step - loss: 1.2811 - accuracy: 0.5902 - val_loss: 1.0539 - val_accuracy: 0.6833\n",
      "Epoch 492/500\n",
      "132/132 [==============================] - 0s 845us/step - loss: 1.2725 - accuracy: 0.5919 - val_loss: 1.0519 - val_accuracy: 0.6900\n",
      "Epoch 493/500\n",
      "132/132 [==============================] - 0s 840us/step - loss: 1.2790 - accuracy: 0.5926 - val_loss: 1.0507 - val_accuracy: 0.6883\n",
      "Epoch 494/500\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.2662 - accuracy: 0.5907 - val_loss: 1.0516 - val_accuracy: 0.6861\n",
      "Epoch 495/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2744 - accuracy: 0.5845 - val_loss: 1.0505 - val_accuracy: 0.6900\n",
      "Epoch 496/500\n",
      "132/132 [==============================] - 0s 844us/step - loss: 1.2965 - accuracy: 0.5783 - val_loss: 1.0608 - val_accuracy: 0.6867\n",
      "Epoch 497/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2630 - accuracy: 0.5907 - val_loss: 1.0560 - val_accuracy: 0.6867\n",
      "Epoch 498/500\n",
      "132/132 [==============================] - 0s 860us/step - loss: 1.2421 - accuracy: 0.6048 - val_loss: 1.0433 - val_accuracy: 0.6867\n",
      "Epoch 499/500\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.2851 - accuracy: 0.5852 - val_loss: 1.0465 - val_accuracy: 0.6922\n",
      "Epoch 500/500\n",
      "132/132 [==============================] - 0s 842us/step - loss: 1.2549 - accuracy: 0.5874 - val_loss: 1.0501 - val_accuracy: 0.6917\n",
      "57/57 [==============================] - 0s 414us/step - loss: 1.0501 - accuracy: 0.6917\n",
      "approximate Test Accuracy: 69.17%\n"
     ]
    }
   ],
   "source": [
    "# define architecture \n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(X_t.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(8, activation='softmax')  # Output layer with 8 neurons for classification\n",
    "])\n",
    "\n",
    "# specify the way of training\n",
    "model.compile(optimizer='adam', # 'adam', 'sgd', 'rmsprop'\n",
    "              loss='sparse_categorical_crossentropy',  #  'sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_t, y_t, epochs=500, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'approximate Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c8fe7",
   "metadata": {},
   "source": [
    "## Export in csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the predictions on the test data in csv format\n",
    "prediction = pd.DataFrame(y_pred, columns=['Genre'])\n",
    "prediction.index.name='Id'\n",
    "prediction.to_csv('myprediction.csv') # export to csv file\n",
    "\n",
    "# The csv file should be of the form\n",
    "#Id, Genre\n",
    "#0, Folk\n",
    "#1, Hip-Hop\n",
    "#2, International\n",
    "#...\n",
    "#1998, Experimental\n",
    "#1999, Pop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
