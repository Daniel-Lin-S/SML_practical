{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb32710c",
   "metadata": {},
   "source": [
    "# SML Pratical\n",
    "\n",
    "Music Genre Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94689a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec509956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and the test inputs\n",
    "X_train = pd.read_csv('X_train.csv', index_col = 0, header=[0, 1, 2]) # inputs of the training set\n",
    "y_train = pd.read_csv('y_train.csv', index_col = 0).squeeze('columns') # outputs of the training set\n",
    "X_test = pd.read_csv('X_test.csv', index_col = 0, header=[0, 1, 2]) # inputs of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbba173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_mapping = {}\n",
    "\n",
    "def transform_labels_to_numbers(labels):\n",
    "    unique_labels = set(labels)\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        class_label_mapping[label] = i\n",
    "\n",
    "    transformed_labels = [class_label_mapping[label] for label in labels]\n",
    "    \n",
    "    return transformed_labels, class_label_mapping\n",
    "\n",
    "y_train, label_mapping = transform_labels_to_numbers(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c1798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total number of rows and columns(attributes)\n",
    "n, p = np.shape(X_train)\n",
    "# Entries (i,j) correspond to the j'th dimension of the observation i\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"unique features: {X_train.columns.get_level_values('feature').unique().tolist()}\")\n",
    "print(f\"statistics used: {X_train.columns.get_level_values('statistics').unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee583d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  (may not be useful) plot correlations for each set of statistics\n",
    "\n",
    "statistics = X_train.columns.get_level_values('statistics').unique()\n",
    "\n",
    "for statistic in statistics:\n",
    "    # obtain the columns for each feature\n",
    "    cols = [col for col in X_train if col[1] == statistic]\n",
    "    # find the correlation matrix\n",
    "    corr = X_train[cols].corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 11))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    plt.title(f'Correlation Heatmap (for {statistic})', fontsize = 25)\n",
    "    plt.xticks(fontsize = 10)\n",
    "    plt.yticks(fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train contains the true class:  Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock\n",
    "classes = np.unique(y_train)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test is the array of test inputs, of the same format as X_train. The objective is to predict the class (Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock) of the output\n",
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b6f37",
   "metadata": {},
   "source": [
    "## Pre-processing and dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "### normalise the training set and test set together ###\n",
    "X = pd.concat([X_train, X_test], ignore_index=True)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
    "X_train_scaled = X_scaled.iloc[:6000,:]\n",
    "X_test_scaled = X_scaled.iloc[6000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390646ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### use PCA to reduce the dimension ###\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=p)\n",
    "# find the principal compoennts\n",
    "pc = pd.DataFrame(data = pca.fit_transform(X_train_scaled), columns = [f'PC {i}' for i in range(1, p+1)])\n",
    "\n",
    "# concatenate labels \n",
    "Df_PCA = pd.concat([pc, y_train], axis=1)\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5efff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 101), explained_variances[:100])\n",
    "plt.title('explained variances by principal components')\n",
    "plt.xlabel('PC index')\n",
    "plt.ylabel('ratio of explained variance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fe7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_PCA = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253bb33",
   "metadata": {},
   "source": [
    "elbow method: take around 20 PCs as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first two principal components (useless plot, messy)\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "sns.scatterplot(x='PC 1', y='PC 2', hue=Df_PCA['Genre'], data=Df_PCA.iloc[:, :2], alpha=0.5)\n",
    "\n",
    "\n",
    "plt.title('Plot of first two components, with the genre represented by colour', fontsize=17)\n",
    "plt.xlabel('first principal component', fontsize=14)\n",
    "plt.xlabel('second principal component', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96abcf",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8af683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try Various Machien Learning Algorithms ###\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from xgboost import plot_tree, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "281d7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(X_train_scaled, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "## use PCA to reduce dimension. n = 20\n",
    "pca = PCA(n_components=n_PCA)\n",
    "X_t_PC = pd.DataFrame(data = pca.fit_transform(X_t), columns = [f'PC {i}' for i in range(1, n_PCA+1)])\n",
    "pca = PCA(n_components=n_PCA)\n",
    "X_val_PC = pd.DataFrame(data = pca.fit_transform(X_val), columns = [f'PC {i}' for i in range(1, n_PCA+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd8e5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_PCA(model, name):\n",
    "    model.fit(X_t_PC, y_t)\n",
    "    y_pred = model.predict(X_val_PC)\n",
    "    print('Validation Accuracy', name, ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f587e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy naive_Bayes : 0.26778 \n",
      "\n",
      "Validation Accuracy SGD : 0.18611 \n",
      "\n",
      "Validation Accuracy Decision_tree : 0.16611 \n",
      "\n",
      "Validation Accuracy random_forest : 0.22778 \n",
      "\n",
      "Validation Accuracy SVM : 0.23167 \n",
      "\n",
      "Validation Accuracy logistic_regression : 0.20778 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linyuhang/mambaforge/envs/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy neural network : 0.19889 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting tree : 0.23222 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting : 0.24167 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## casual trainings with no tuning\n",
    "nb =  GaussianNB()\n",
    "sgd = SGDClassifier(max_iter=4000)\n",
    "tree = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20)\n",
    "svm = SVC(decision_function_shape=\"ovo\")\n",
    "lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(200, 10), random_state=1)\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.04)\n",
    "xgbrf = XGBRFClassifier(objective= 'multi:softmax')\n",
    "\n",
    "algorithms = {\n",
    "    'naive_Bayes': nb,\n",
    "    'SGD' : sgd,\n",
    "    'Decision_tree': tree,\n",
    "    'random_forest': rf,\n",
    "    'SVM': svm,\n",
    "    'logistic_regression': lg,\n",
    "    'neural network': nn,\n",
    "    'cross-gradient boosting tree': xgb,\n",
    "    'cross-gradient boosting': xgbrf\n",
    "}\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    model_PCA(algorithm, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ade29",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8709f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 0.54833 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Use LDA to reduce dimension instead ###\n",
    "LDAclassifier = LinearDiscriminantAnalysis(n_components=7) # 7 classes in total \n",
    "LDAclassifier.fit(X_t, y_t)\n",
    "y_pred = LDAclassifier.predict(X_val)\n",
    "print('Validation Accuracy', ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "096b10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LDA(model, name):\n",
    "    \"\"\"training based on LDA for dimension-reduction\"\"\"\n",
    "    # obtain LDA components for other algorithms\n",
    "    LDA = LinearDiscriminantAnalysis()\n",
    "    LDA.fit(X_t, y_t)\n",
    "    X_t_LDA = LDA.transform(X_t)\n",
    "    X_val_LDA = LDA.transform(X_val)      \n",
    "    model.fit(X_t_LDA, y_t)\n",
    "    y_pred = model.predict(X_val_LDA)\n",
    "    print('Validation Accuracy', name, ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e6862a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy naive_Bayes : 0.55833 \n",
      "\n",
      "Validation Accuracy SGD : 0.53278 \n",
      "\n",
      "Validation Accuracy Decision_tree : 0.44833 \n",
      "\n",
      "Validation Accuracy random_forest : 0.55167 \n",
      "\n",
      "Validation Accuracy SVM : 0.55722 \n",
      "\n",
      "Validation Accuracy logistic_regression : 0.54611 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linyuhang/mambaforge/envs/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy neural network : 0.50556 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting tree : 0.54556 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting : 0.54333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, algorithm in algorithms.items():\n",
    "    model_LDA(algorithm, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bcee2",
   "metadata": {},
   "source": [
    "Conclusion: LDA is better than PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33bb35",
   "metadata": {},
   "source": [
    "## Cross-validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d204196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ed185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# use LDA to reduce dimensions\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train_scaled, y_train)\n",
    "X_train_LDA = LDA.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82f47920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD tuning: regularisation strength, penalty and loss \n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'loss': ['huber', 'squared_error', 'hinge', 'perceptron', 'epsilon_insensitive', 'log_loss', 'squared_hinge', 'squared_epsilon_insensitive', 'modified_huber']\n",
    "}\n",
    "\n",
    "sgd = SGDClassifier(max_iter=5000)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    grid_SGD = GridSearchCV(sgd, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_SGD.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ea664e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6898333333333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(grid_SGD.best_params_)\n",
    "grid_SGD.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaef554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest tuning:  \n",
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 500, 1000], # number of trees\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [5, 10, 20], # minimum size of node for splitting\n",
    "    'min_samples_leaf': [1, 2, 4],  # minimum leaf size \n",
    "    'max_features': ['auto', 'sqrt', 'log2', None] # number of features used in each tree\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_RF = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_RF.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513636d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_RF.best_params_)\n",
    "grid_RF.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691bc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning SVM\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10], # regularisation strength\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], \n",
    "    'gamma': ['scale', 'auto', 0.1, 1], # RBF parameter\n",
    "    'degree': [4, 10, 20] # only for poly, degree of polynomial\n",
    "}\n",
    "\n",
    "# Create the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_SVM = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_SVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b841ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_SVM.best_params_)\n",
    "grid_SVM.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tuning logistic regression\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], # regularisation strength\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cg']  # solver for optimisation\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_logit = GridSearchCV(logreg_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e44de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_logit.best_params_)\n",
    "grid_logit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning MLP\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50), (100, 100), (200, 200),\n",
    "                           (50, 50, 50), (100, 100, 100), (200, 200, 200)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # regularisation strength\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Create the MLPClassifier\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_mlp = GridSearchCV(mlp_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_mlp.best_params_)\n",
    "grid_mlp.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95025b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunig gradient boosting random forest\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2], # step size shrinkage used in each boosting iteration\n",
    "    'n_estimators': [100, 200, 500, 1000], # number of trees\n",
    "    'max_depth': [None, 5, 7, 10, 20], # max tree depth\n",
    "    'subsample': [0.6, 0.8, 1.0], # fraction of samples used for fitting each tree\n",
    "    'colsample_bynode': [0.4, 0.6, 0.8, 1.0], # fraction of features used for fitting each tree\n",
    "    'gamma': [0, 0.1, 0.2], # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "}\n",
    "\n",
    "# Create the XGBRFClassifier\n",
    "xgbrf_classifier = XGBRFClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(xgbrf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c8fe7",
   "metadata": {},
   "source": [
    "## Export in csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the predictions on the test data in csv format\n",
    "prediction = pd.DataFrame(y_pred, columns=['Genre'])\n",
    "prediction.index.name='Id'\n",
    "prediction.to_csv('myprediction.csv') # export to csv file\n",
    "\n",
    "# The csv file should be of the form\n",
    "#Id, Genre\n",
    "#0, Folk\n",
    "#1, Hip-Hop\n",
    "#2, International\n",
    "#...\n",
    "#1998, Experimental\n",
    "#1999, Pop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
