{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb32710c",
   "metadata": {},
   "source": [
    "# SML Pratical\n",
    "\n",
    "Music Genre Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5c7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94689a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec509956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and the test inputs\n",
    "X_train = pd.read_csv('data/X_train.csv', index_col = 0, header=[0, 1, 2]) # inputs of the training set\n",
    "y_train = pd.read_csv('data/y_train.csv', index_col = 0).squeeze('columns') # outputs of the training set\n",
    "X_test = pd.read_csv('data/X_test.csv', index_col = 0, header=[0, 1, 2]) # inputs of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbba173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_mapping = {}\n",
    "\n",
    "def transform_labels_to_numbers(labels):\n",
    "    unique_labels = set(labels)\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        class_label_mapping[label] = i\n",
    "\n",
    "    transformed_labels = [class_label_mapping[label] for label in labels]\n",
    "    \n",
    "    return transformed_labels, class_label_mapping\n",
    "\n",
    "y_train, label_mapping = transform_labels_to_numbers(y_train)\n",
    "y_train = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135c1798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th colspan=\"10\" halign=\"left\">chroma_cens</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">tonnetz</th>\n",
       "      <th colspan=\"7\" halign=\"left\">zcr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistics</th>\n",
       "      <th colspan=\"10\" halign=\"left\">kurtosis</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">std</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>skew</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "      <th>01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.266585</td>\n",
       "      <td>-0.984668</td>\n",
       "      <td>-0.729823</td>\n",
       "      <td>-0.895122</td>\n",
       "      <td>2.138628</td>\n",
       "      <td>0.935209</td>\n",
       "      <td>0.104089</td>\n",
       "      <td>-0.698659</td>\n",
       "      <td>-0.736408</td>\n",
       "      <td>-0.334376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065003</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>5.743597</td>\n",
       "      <td>0.307617</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>1.976972</td>\n",
       "      <td>0.034533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.180061</td>\n",
       "      <td>0.260884</td>\n",
       "      <td>-0.069373</td>\n",
       "      <td>0.208734</td>\n",
       "      <td>-0.078855</td>\n",
       "      <td>-0.577818</td>\n",
       "      <td>0.583788</td>\n",
       "      <td>0.143781</td>\n",
       "      <td>0.291556</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087692</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.016605</td>\n",
       "      <td>64.870987</td>\n",
       "      <td>0.812988</td>\n",
       "      <td>0.082784</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>7.374503</td>\n",
       "      <td>0.074870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.692900</td>\n",
       "      <td>0.356662</td>\n",
       "      <td>0.062617</td>\n",
       "      <td>0.248280</td>\n",
       "      <td>3.470037</td>\n",
       "      <td>0.166613</td>\n",
       "      <td>0.823874</td>\n",
       "      <td>0.181112</td>\n",
       "      <td>0.551939</td>\n",
       "      <td>0.357985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132387</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>34.251705</td>\n",
       "      <td>0.850098</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>5.927942</td>\n",
       "      <td>0.117603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.243339</td>\n",
       "      <td>0.214182</td>\n",
       "      <td>-0.049026</td>\n",
       "      <td>1.456255</td>\n",
       "      <td>-0.360826</td>\n",
       "      <td>-0.875256</td>\n",
       "      <td>-0.770200</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>0.789956</td>\n",
       "      <td>0.448319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071478</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>1.364990</td>\n",
       "      <td>0.342285</td>\n",
       "      <td>0.081713</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100437</td>\n",
       "      <td>0.041754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.968576</td>\n",
       "      <td>0.309255</td>\n",
       "      <td>0.223164</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.919838</td>\n",
       "      <td>-0.111985</td>\n",
       "      <td>-1.012521</td>\n",
       "      <td>-0.665692</td>\n",
       "      <td>-0.316646</td>\n",
       "      <td>-0.264381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106220</td>\n",
       "      <td>0.023536</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>3.589230</td>\n",
       "      <td>0.322266</td>\n",
       "      <td>0.073736</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>1.210593</td>\n",
       "      <td>0.036459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>0.503490</td>\n",
       "      <td>-0.540720</td>\n",
       "      <td>-0.690117</td>\n",
       "      <td>-0.107338</td>\n",
       "      <td>-0.647856</td>\n",
       "      <td>-0.681969</td>\n",
       "      <td>-0.246245</td>\n",
       "      <td>-0.546552</td>\n",
       "      <td>0.062783</td>\n",
       "      <td>0.070393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084929</td>\n",
       "      <td>0.017250</td>\n",
       "      <td>0.020335</td>\n",
       "      <td>4.868783</td>\n",
       "      <td>0.668945</td>\n",
       "      <td>0.076452</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>2.045856</td>\n",
       "      <td>0.084214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>-0.600597</td>\n",
       "      <td>0.406386</td>\n",
       "      <td>-0.748409</td>\n",
       "      <td>-0.316157</td>\n",
       "      <td>-0.507428</td>\n",
       "      <td>-0.054214</td>\n",
       "      <td>-0.476804</td>\n",
       "      <td>-0.373120</td>\n",
       "      <td>-0.930158</td>\n",
       "      <td>-1.080690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075407</td>\n",
       "      <td>0.014998</td>\n",
       "      <td>0.020683</td>\n",
       "      <td>7.893681</td>\n",
       "      <td>0.584961</td>\n",
       "      <td>0.076210</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.561808</td>\n",
       "      <td>0.073010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>-1.014298</td>\n",
       "      <td>-0.950744</td>\n",
       "      <td>0.618304</td>\n",
       "      <td>0.204298</td>\n",
       "      <td>-0.788411</td>\n",
       "      <td>-0.794254</td>\n",
       "      <td>-0.586847</td>\n",
       "      <td>0.099172</td>\n",
       "      <td>-0.313476</td>\n",
       "      <td>-0.523417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138591</td>\n",
       "      <td>0.024969</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>27.257378</td>\n",
       "      <td>0.373047</td>\n",
       "      <td>0.042598</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.778109</td>\n",
       "      <td>0.027813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>-0.002938</td>\n",
       "      <td>0.646034</td>\n",
       "      <td>-0.732819</td>\n",
       "      <td>1.205990</td>\n",
       "      <td>-0.898733</td>\n",
       "      <td>-0.684953</td>\n",
       "      <td>0.134642</td>\n",
       "      <td>-0.374792</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>-1.016032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>0.030371</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>431.200500</td>\n",
       "      <td>0.384277</td>\n",
       "      <td>0.025731</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>10.260160</td>\n",
       "      <td>0.006870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>-0.881917</td>\n",
       "      <td>-0.615638</td>\n",
       "      <td>0.891155</td>\n",
       "      <td>4.850262</td>\n",
       "      <td>-0.349280</td>\n",
       "      <td>-1.010980</td>\n",
       "      <td>-0.577905</td>\n",
       "      <td>-0.046604</td>\n",
       "      <td>-0.828026</td>\n",
       "      <td>-0.457468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175120</td>\n",
       "      <td>0.039703</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>13.567758</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.047689</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>2.553229</td>\n",
       "      <td>0.018049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 518 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature    chroma_cens                                                    \\\n",
       "statistics    kurtosis                                                     \n",
       "number              01        02        03        04        05        06   \n",
       "Id                                                                         \n",
       "0            -0.266585 -0.984668 -0.729823 -0.895122  2.138628  0.935209   \n",
       "1            -0.180061  0.260884 -0.069373  0.208734 -0.078855 -0.577818   \n",
       "2            -0.692900  0.356662  0.062617  0.248280  3.470037  0.166613   \n",
       "3             0.243339  0.214182 -0.049026  1.456255 -0.360826 -0.875256   \n",
       "4            -0.968576  0.309255  0.223164  0.160960  0.919838 -0.111985   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "5995          0.503490 -0.540720 -0.690117 -0.107338 -0.647856 -0.681969   \n",
       "5996         -0.600597  0.406386 -0.748409 -0.316157 -0.507428 -0.054214   \n",
       "5997         -1.014298 -0.950744  0.618304  0.204298 -0.788411 -0.794254   \n",
       "5998         -0.002938  0.646034 -0.732819  1.205990 -0.898733 -0.684953   \n",
       "5999         -0.881917 -0.615638  0.891155  4.850262 -0.349280 -1.010980   \n",
       "\n",
       "feature                                             ...   tonnetz            \\\n",
       "statistics                                          ...       std             \n",
       "number            07        08        09        10  ...        04        05   \n",
       "Id                                                  ...                       \n",
       "0           0.104089 -0.698659 -0.736408 -0.334376  ...  0.065003  0.016522   \n",
       "1           0.583788  0.143781  0.291556  0.007314  ...  0.087692  0.016355   \n",
       "2           0.823874  0.181112  0.551939  0.357985  ...  0.132387  0.025847   \n",
       "3          -0.770200  0.315500  0.789956  0.448319  ...  0.071478  0.019166   \n",
       "4          -1.012521 -0.665692 -0.316646 -0.264381  ...  0.106220  0.023536   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "5995       -0.246245 -0.546552  0.062783  0.070393  ...  0.084929  0.017250   \n",
       "5996       -0.476804 -0.373120 -0.930158 -1.080690  ...  0.075407  0.014998   \n",
       "5997       -0.586847  0.099172 -0.313476 -0.523417  ...  0.138591  0.024969   \n",
       "5998        0.134642 -0.374792 -0.019524 -1.016032  ...  0.137695  0.030371   \n",
       "5999       -0.577905 -0.046604 -0.828026 -0.457468  ...  0.175120  0.039703   \n",
       "\n",
       "feature                      zcr                                          \\\n",
       "statistics              kurtosis       max      mean    median       min   \n",
       "number            06          01        01        01        01        01   \n",
       "Id                                                                         \n",
       "0           0.015776    5.743597  0.307617  0.051370  0.042480  0.002441   \n",
       "1           0.016605   64.870987  0.812988  0.082784  0.069824  0.003906   \n",
       "2           0.023922   34.251705  0.850098  0.058200  0.036621  0.010254   \n",
       "3           0.025535    1.364990  0.342285  0.081713  0.075195  0.000000   \n",
       "4           0.019742    3.589230  0.322266  0.073736  0.069336  0.004395   \n",
       "...              ...         ...       ...       ...       ...       ...   \n",
       "5995        0.020335    4.868783  0.668945  0.076452  0.044434  0.001465   \n",
       "5996        0.020683    7.893681  0.584961  0.076210  0.048340  0.000000   \n",
       "5997        0.023658   27.257378  0.373047  0.042598  0.037598  0.000000   \n",
       "5998        0.029970  431.200500  0.384277  0.025731  0.025391  0.008301   \n",
       "5999        0.025094   13.567758  0.233398  0.047689  0.044922  0.015625   \n",
       "\n",
       "feature                          \n",
       "statistics       skew       std  \n",
       "number             01        01  \n",
       "Id                               \n",
       "0            1.976972  0.034533  \n",
       "1            7.374503  0.074870  \n",
       "2            5.927942  0.117603  \n",
       "3            1.100437  0.041754  \n",
       "4            1.210593  0.036459  \n",
       "...               ...       ...  \n",
       "5995         2.045856  0.084214  \n",
       "5996         2.561808  0.073010  \n",
       "5997         3.778109  0.027813  \n",
       "5998        10.260160  0.006870  \n",
       "5999         2.553229  0.018049  \n",
       "\n",
       "[6000 rows x 518 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of rows and columns(attributes)\n",
    "n, p = np.shape(X_train)\n",
    "# Entries (i,j) correspond to the j'th dimension of the observation i\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd13ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique features: ['chroma_cens', 'chroma_cqt', 'chroma_stft', 'mfcc', 'rmse', 'spectral_bandwidth', 'spectral_centroid', 'spectral_contrast', 'spectral_rolloff', 'tonnetz', 'zcr']\n",
      "statistics used: ['kurtosis', 'max', 'mean', 'median', 'min', 'skew', 'std']\n"
     ]
    }
   ],
   "source": [
    "print(f\"unique features: {X_train.columns.get_level_values('feature').unique().tolist()}\")\n",
    "print(f\"statistics used: {X_train.columns.get_level_values('statistics').unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee583d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  (may not be useful) plot correlations for each set of statistics\n",
    "\n",
    "statistics = X_train.columns.get_level_values('statistics').unique()\n",
    "\n",
    "for statistic in statistics:\n",
    "    # obtain the columns for each feature\n",
    "    cols = [col for col in X_train if col[1] == statistic]\n",
    "    # find the correlation matrix\n",
    "    corr = X_train[cols].corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 11))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    plt.title(f'Correlation Heatmap (for {statistic})', fontsize = 25)\n",
    "    plt.xticks(fontsize = 10)\n",
    "    plt.yticks(fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5cdefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train contains the true class:  Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock\n",
    "classes = np.unique(y_train)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551a7367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 518)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test is the array of test inputs, of the same format as X_train. The objective is to predict the class (Electronic, Experimental, Folk, Hip-Hop, Instrumental, International, Pop or Rock) of the output\n",
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32c045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "### normalise the training set and test set together ###\n",
    "X = pd.concat([X_train, X_test], ignore_index=True)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
    "X_train_scaled = X_scaled.iloc[:6000,:]\n",
    "X_test_scaled = X_scaled.iloc[6000:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4253d",
   "metadata": {},
   "source": [
    "## PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390646ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### use PCA to reduce the dimension ###\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=p)\n",
    "# find the principal compoennts\n",
    "pc = pd.DataFrame(data = pca.fit_transform(X_train_scaled), columns = [f'PC {i}' for i in range(1, p+1)])\n",
    "\n",
    "# concatenate labels \n",
    "Df_PCA = pd.concat([pc, y_train], axis=1)\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5efff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 101), explained_variances[:100])\n",
    "plt.title('explained variances by principal components')\n",
    "plt.xlabel('PC index')\n",
    "plt.ylabel('ratio of explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fe7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_PCA = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253bb33",
   "metadata": {},
   "source": [
    "elbow method: take around 20 PCs as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first two principal components (useless plot, messy)\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "sns.scatterplot(x='PC 1', y='PC 2', hue=Df_PCA['Genre'], data=Df_PCA.iloc[:, :2], alpha=0.5)\n",
    "\n",
    "\n",
    "plt.title('Plot of first two components, with the genre represented by colour', fontsize=17)\n",
    "plt.xlabel('first principal component', fontsize=14)\n",
    "plt.xlabel('second principal component', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96abcf",
   "metadata": {},
   "source": [
    "## Classical Training Models\n",
    "Naive Bayes, two-layer perceptron, linear SVM, kernel SVM, random forests (and with gradient boosting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8af683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try Various Machien Learning Algorithms ###\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from xgboost import plot_tree, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### casual trainings with no tuning ###\n",
    "nb =  GaussianNB()\n",
    "sgd = SGDClassifier(max_iter=4000)\n",
    "tree = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20)\n",
    "svm = SVC(decision_function_shape=\"ovo\")\n",
    "lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(200, 10), random_state=1)\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.04)\n",
    "xgbrf = XGBRFClassifier(objective= 'multi:softmax')\n",
    "\n",
    "algorithms = {\n",
    "    'naive_Bayes': nb,\n",
    "    'SGD' : sgd,\n",
    "    'Decision_tree': tree,\n",
    "    'random_forest': rf,\n",
    "    'SVM': svm,\n",
    "    'logistic_regression': lg,\n",
    "    'neural network': nn,\n",
    "    'cross-gradient boosting tree': xgb,\n",
    "    'cross-gradient boosting': xgbrf\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c541f1",
   "metadata": {},
   "source": [
    "### PCA for dimension-reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(X_train_scaled, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "## use PCA to reduce dimension. n = 20\n",
    "pca = PCA(n_components=n_PCA)\n",
    "X_t_PC = pd.DataFrame(data = pca.fit_transform(X_t), columns = [f'PC {i}' for i in range(1, n_PCA+1)])\n",
    "pca = PCA(n_components=n_PCA)\n",
    "X_val_PC = pd.DataFrame(data = pca.fit_transform(X_val), columns = [f'PC {i}' for i in range(1, n_PCA+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_PCA(model, name):\n",
    "    model.fit(X_t_PC, y_t)\n",
    "    y_pred = model.predict(X_val_PC)\n",
    "    print('Validation Accuracy', name, ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f587e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy naive_Bayes : 0.26778 \n",
      "\n",
      "Validation Accuracy SGD : 0.18611 \n",
      "\n",
      "Validation Accuracy Decision_tree : 0.16611 \n",
      "\n",
      "Validation Accuracy random_forest : 0.22778 \n",
      "\n",
      "Validation Accuracy SVM : 0.23167 \n",
      "\n",
      "Validation Accuracy logistic_regression : 0.20778 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linyuhang/mambaforge/envs/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy neural network : 0.19889 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting tree : 0.23222 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting : 0.24167 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, algorithm in algorithms.items():\n",
    "    model_PCA(algorithm, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ade29",
   "metadata": {},
   "source": [
    "### Fisher's LDA for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8709f32a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Use LDA classifier ###\u001b[39;00m\n\u001b[1;32m      2\u001b[0m LDAclassifier \u001b[38;5;241m=\u001b[39m LinearDiscriminantAnalysis(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m) \u001b[38;5;66;03m# 7 classes in total \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m LDAclassifier\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_t\u001b[49m, y_t)\n\u001b[1;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m LDAclassifier\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mround\u001b[39m(accuracy_score(y_val, y_pred), \u001b[38;5;241m5\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_t' is not defined"
     ]
    }
   ],
   "source": [
    "### Use LDA classifier ###\n",
    "LDAclassifier = LinearDiscriminantAnalysis(n_components=7) # 7 classes in total \n",
    "LDAclassifier.fit(X_t, y_t)\n",
    "y_pred = LDAclassifier.predict(X_val)\n",
    "print('Validation Accuracy', ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use LDA to reduce dimension, and apply other algorithms ###\n",
    "\n",
    "def model_LDA(model, name):\n",
    "    \"\"\"training based on LDA for dimension-reduction\"\"\"\n",
    "    # obtain LDA components for other algorithms\n",
    "    LDA = LinearDiscriminantAnalysis()\n",
    "    LDA.fit(X_t, y_t)\n",
    "    X_t_LDA = LDA.transform(X_t)\n",
    "    X_val_LDA = LDA.transform(X_val)      \n",
    "    model.fit(X_t_LDA, y_t)\n",
    "    y_pred = model.predict(X_val_LDA)\n",
    "    print('Validation Accuracy', name, ':', round(accuracy_score(y_val, y_pred), 5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6862a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy naive_Bayes : 0.55833 \n",
      "\n",
      "Validation Accuracy SGD : 0.53278 \n",
      "\n",
      "Validation Accuracy Decision_tree : 0.44833 \n",
      "\n",
      "Validation Accuracy random_forest : 0.55167 \n",
      "\n",
      "Validation Accuracy SVM : 0.55722 \n",
      "\n",
      "Validation Accuracy logistic_regression : 0.54611 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linyuhang/mambaforge/envs/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy neural network : 0.50556 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting tree : 0.54556 \n",
      "\n",
      "Validation Accuracy cross-gradient boosting : 0.54333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, algorithm in algorithms.items():\n",
    "    model_LDA(algorithm, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bcee2",
   "metadata": {},
   "source": [
    "Conclusion: LDA is better than PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33bb35",
   "metadata": {},
   "source": [
    "## Cross-validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d204196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f48ed185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LDA to reduce dimensions\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train_scaled, y_train)\n",
    "X_train_LDA = LDA.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f47920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001, 'loss': 'log_loss', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD tuning: regularisation strength, penalty and loss \n",
    "sgd = SGDClassifier(max_iter=5000)\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'loss': list(sgd.loss_functions)\n",
    "}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "    grid_SGD = GridSearchCV(sgd, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_SGD.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea664e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6898333333333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(grid_SGD.best_params_)\n",
    "grid_SGD.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaef554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest tuning:  \n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100], # number of trees\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [8, 10, 20], # minimum size of node for splitting\n",
    "    'min_samples_leaf': [2, 4, 6],  # minimum leaf size \n",
    "    'max_features': ['auto', 'sqrt', 'log2', None] # number of features used in each tree\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_RF = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_RF.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513636d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_RF.best_params_)\n",
    "grid_RF.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tuning logistic regression\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], # regularisation strength\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cg']  # solver for optimisation\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_logit = GridSearchCV(logreg_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_logit.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e44de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_logit.best_params_)\n",
    "grid_logit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95025b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunig gradient boosting random forest\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2], # step size shrinkage used in each boosting iteration\n",
    "    'n_estimators': [50, 100], # number of trees\n",
    "    'max_depth': [5, 7, 10], # max tree depth\n",
    "    'subsample': [0.6, 0.8], # fraction of samples used for fitting each tree\n",
    "    'colsample_bynode': [0.6, 0.8], # fraction of features used for fitting each tree\n",
    "    'gamma': [0, 0.1, 0.2], # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "}\n",
    "\n",
    "# Create the XGBRFClassifier\n",
    "xgbrf_classifier = XGBRFClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_xgb = GridSearchCV(xgbrf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_xgb.fit(X_train_LDA, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beae048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_xgb.best_params_)\n",
    "grid_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20238d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tuning Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Define the parameter grid for AdaBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'base_estimator__max_depth': [2, 4, 6] \n",
    "}\n",
    "\n",
    "# Create the AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(base_estimator=base_estimator)\n",
    "\n",
    "# grid search\n",
    "grid_ada = GridSearchCV(adaboost, param_grid, cv=5, scoring='accuracy')\n",
    "grid_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_ada.best_params_)\n",
    "grid_ada.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46597438",
   "metadata": {},
   "source": [
    "## MLP training using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88fa4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0b98d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LDA to reduce dimensions\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train_scaled, y_train)\n",
    "X_train_LDA = LDA.transform(X_train_scaled)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train_LDA, y_train, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ee6dbad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.6621 - accuracy: 0.4188 - val_loss: 1.0747 - val_accuracy: 0.6806\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 775us/step - loss: 1.2592 - accuracy: 0.5829 - val_loss: 0.9421 - val_accuracy: 0.6889\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 870us/step - loss: 1.1683 - accuracy: 0.6145 - val_loss: 0.9115 - val_accuracy: 0.6989\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 821us/step - loss: 1.1372 - accuracy: 0.6214 - val_loss: 0.9084 - val_accuracy: 0.6972\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 788us/step - loss: 1.0908 - accuracy: 0.6352 - val_loss: 0.8982 - val_accuracy: 0.7044\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 776us/step - loss: 1.1019 - accuracy: 0.6369 - val_loss: 0.8961 - val_accuracy: 0.6983\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0925 - accuracy: 0.6400 - val_loss: 0.8991 - val_accuracy: 0.7006\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 768us/step - loss: 1.0767 - accuracy: 0.6531 - val_loss: 0.8957 - val_accuracy: 0.6978\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 1.0558 - accuracy: 0.6495 - val_loss: 0.8923 - val_accuracy: 0.7011\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 1.0607 - accuracy: 0.6474 - val_loss: 0.8893 - val_accuracy: 0.6950\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 1.0514 - accuracy: 0.6529 - val_loss: 0.8934 - val_accuracy: 0.7022\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 784us/step - loss: 1.0705 - accuracy: 0.6517 - val_loss: 0.8944 - val_accuracy: 0.6983\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 847us/step - loss: 1.0542 - accuracy: 0.6576 - val_loss: 0.8915 - val_accuracy: 0.6983\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 855us/step - loss: 1.0481 - accuracy: 0.6533 - val_loss: 0.8945 - val_accuracy: 0.7022\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 819us/step - loss: 1.0461 - accuracy: 0.6502 - val_loss: 0.8970 - val_accuracy: 0.6956\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 795us/step - loss: 1.0405 - accuracy: 0.6583 - val_loss: 0.8929 - val_accuracy: 0.6994\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 792us/step - loss: 1.0302 - accuracy: 0.6588 - val_loss: 0.8936 - val_accuracy: 0.6994\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 785us/step - loss: 1.0496 - accuracy: 0.6526 - val_loss: 0.8946 - val_accuracy: 0.6989\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 802us/step - loss: 1.0422 - accuracy: 0.6526 - val_loss: 0.8920 - val_accuracy: 0.7022\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 862us/step - loss: 1.0429 - accuracy: 0.6624 - val_loss: 0.8908 - val_accuracy: 0.7017\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 839us/step - loss: 1.0316 - accuracy: 0.6560 - val_loss: 0.8934 - val_accuracy: 0.6994\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 813us/step - loss: 1.0304 - accuracy: 0.6624 - val_loss: 0.8931 - val_accuracy: 0.7017\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 765us/step - loss: 1.0302 - accuracy: 0.6629 - val_loss: 0.8913 - val_accuracy: 0.7061\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 766us/step - loss: 1.0248 - accuracy: 0.6569 - val_loss: 0.8879 - val_accuracy: 0.7083\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 776us/step - loss: 1.0353 - accuracy: 0.6655 - val_loss: 0.8913 - val_accuracy: 0.7006\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0273 - accuracy: 0.6631 - val_loss: 0.8939 - val_accuracy: 0.7011\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 808us/step - loss: 1.0280 - accuracy: 0.6600 - val_loss: 0.8881 - val_accuracy: 0.6983\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 774us/step - loss: 1.0157 - accuracy: 0.6586 - val_loss: 0.8907 - val_accuracy: 0.7044\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 774us/step - loss: 1.0300 - accuracy: 0.6669 - val_loss: 0.8893 - val_accuracy: 0.7028\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 1.0186 - accuracy: 0.6548 - val_loss: 0.8860 - val_accuracy: 0.6989\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 755us/step - loss: 1.0091 - accuracy: 0.6660 - val_loss: 0.8904 - val_accuracy: 0.6978\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 759us/step - loss: 1.0018 - accuracy: 0.6660 - val_loss: 0.8931 - val_accuracy: 0.6989\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 759us/step - loss: 1.0122 - accuracy: 0.6586 - val_loss: 0.8900 - val_accuracy: 0.7022\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 764us/step - loss: 1.0157 - accuracy: 0.6683 - val_loss: 0.8888 - val_accuracy: 0.7061\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 780us/step - loss: 1.0111 - accuracy: 0.6619 - val_loss: 0.8860 - val_accuracy: 0.7039\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 779us/step - loss: 0.9902 - accuracy: 0.6714 - val_loss: 0.8895 - val_accuracy: 0.7000\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 774us/step - loss: 1.0057 - accuracy: 0.6686 - val_loss: 0.8875 - val_accuracy: 0.7061\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 776us/step - loss: 1.0109 - accuracy: 0.6645 - val_loss: 0.8874 - val_accuracy: 0.7017\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 1.0033 - accuracy: 0.6700 - val_loss: 0.8893 - val_accuracy: 0.7006\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9982 - accuracy: 0.6650 - val_loss: 0.8883 - val_accuracy: 0.7011\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 751us/step - loss: 0.9979 - accuracy: 0.6693 - val_loss: 0.8860 - val_accuracy: 0.7078\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 768us/step - loss: 1.0000 - accuracy: 0.6702 - val_loss: 0.8882 - val_accuracy: 0.7039\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9855 - accuracy: 0.6686 - val_loss: 0.8883 - val_accuracy: 0.7028\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0040 - accuracy: 0.6662 - val_loss: 0.8909 - val_accuracy: 0.7033\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 772us/step - loss: 0.9873 - accuracy: 0.6690 - val_loss: 0.8875 - val_accuracy: 0.7061\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 767us/step - loss: 0.9984 - accuracy: 0.6671 - val_loss: 0.8904 - val_accuracy: 0.7050\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9962 - accuracy: 0.6679 - val_loss: 0.8879 - val_accuracy: 0.7094\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 761us/step - loss: 0.9926 - accuracy: 0.6664 - val_loss: 0.8899 - val_accuracy: 0.6989\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 796us/step - loss: 0.9972 - accuracy: 0.6636 - val_loss: 0.8861 - val_accuracy: 0.6978\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 770us/step - loss: 0.9973 - accuracy: 0.6664 - val_loss: 0.8895 - val_accuracy: 0.6994\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 764us/step - loss: 0.9934 - accuracy: 0.6621 - val_loss: 0.8882 - val_accuracy: 0.7044\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 763us/step - loss: 0.9922 - accuracy: 0.6636 - val_loss: 0.8912 - val_accuracy: 0.6933\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 765us/step - loss: 0.9908 - accuracy: 0.6631 - val_loss: 0.8901 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 766us/step - loss: 0.9960 - accuracy: 0.6702 - val_loss: 0.8877 - val_accuracy: 0.7039\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 764us/step - loss: 0.9919 - accuracy: 0.6729 - val_loss: 0.8873 - val_accuracy: 0.6989\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9889 - accuracy: 0.6676 - val_loss: 0.8887 - val_accuracy: 0.7100\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 761us/step - loss: 0.9926 - accuracy: 0.6736 - val_loss: 0.8874 - val_accuracy: 0.7022\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9799 - accuracy: 0.6693 - val_loss: 0.8882 - val_accuracy: 0.7061\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.9769 - accuracy: 0.6740 - val_loss: 0.8899 - val_accuracy: 0.7039\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9809 - accuracy: 0.6740 - val_loss: 0.8876 - val_accuracy: 0.7017\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 767us/step - loss: 0.9722 - accuracy: 0.6669 - val_loss: 0.8892 - val_accuracy: 0.7028\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9747 - accuracy: 0.6757 - val_loss: 0.8876 - val_accuracy: 0.6978\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9848 - accuracy: 0.6700 - val_loss: 0.8878 - val_accuracy: 0.7044\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9801 - accuracy: 0.6712 - val_loss: 0.8917 - val_accuracy: 0.7061\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 759us/step - loss: 0.9865 - accuracy: 0.6690 - val_loss: 0.8902 - val_accuracy: 0.7022\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 764us/step - loss: 0.9717 - accuracy: 0.6700 - val_loss: 0.8862 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 764us/step - loss: 0.9577 - accuracy: 0.6717 - val_loss: 0.8867 - val_accuracy: 0.7028\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 759us/step - loss: 0.9794 - accuracy: 0.6681 - val_loss: 0.8887 - val_accuracy: 0.7039\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9704 - accuracy: 0.6767 - val_loss: 0.8828 - val_accuracy: 0.7039\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 0.9909 - accuracy: 0.6733 - val_loss: 0.8881 - val_accuracy: 0.7056\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9838 - accuracy: 0.6660 - val_loss: 0.8900 - val_accuracy: 0.7050\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 0.9799 - accuracy: 0.6636 - val_loss: 0.8900 - val_accuracy: 0.7039\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 761us/step - loss: 0.9839 - accuracy: 0.6690 - val_loss: 0.8889 - val_accuracy: 0.7011\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9812 - accuracy: 0.6731 - val_loss: 0.8885 - val_accuracy: 0.7039\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 755us/step - loss: 0.9658 - accuracy: 0.6736 - val_loss: 0.8903 - val_accuracy: 0.7061\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 0.9665 - accuracy: 0.6700 - val_loss: 0.8883 - val_accuracy: 0.6989\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 761us/step - loss: 0.9824 - accuracy: 0.6688 - val_loss: 0.8898 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 761us/step - loss: 0.9670 - accuracy: 0.6695 - val_loss: 0.8944 - val_accuracy: 0.7006\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9788 - accuracy: 0.6724 - val_loss: 0.8893 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.9846 - accuracy: 0.6669 - val_loss: 0.8915 - val_accuracy: 0.7039\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 783us/step - loss: 0.9742 - accuracy: 0.6729 - val_loss: 0.8882 - val_accuracy: 0.7044\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9709 - accuracy: 0.6721 - val_loss: 0.8955 - val_accuracy: 0.7022\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9751 - accuracy: 0.6717 - val_loss: 0.8908 - val_accuracy: 0.7017\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 0.9713 - accuracy: 0.6788 - val_loss: 0.8911 - val_accuracy: 0.7039\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 754us/step - loss: 0.9646 - accuracy: 0.6731 - val_loss: 0.8917 - val_accuracy: 0.7044\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9584 - accuracy: 0.6710 - val_loss: 0.8925 - val_accuracy: 0.7022\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 756us/step - loss: 0.9679 - accuracy: 0.6783 - val_loss: 0.8924 - val_accuracy: 0.7078\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 0.9712 - accuracy: 0.6790 - val_loss: 0.8921 - val_accuracy: 0.7056\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 766us/step - loss: 0.9696 - accuracy: 0.6724 - val_loss: 0.8922 - val_accuracy: 0.7017\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 767us/step - loss: 0.9696 - accuracy: 0.6788 - val_loss: 0.8926 - val_accuracy: 0.7044\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 760us/step - loss: 0.9677 - accuracy: 0.6707 - val_loss: 0.8927 - val_accuracy: 0.7044\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9595 - accuracy: 0.6807 - val_loss: 0.8930 - val_accuracy: 0.7039\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9655 - accuracy: 0.6750 - val_loss: 0.8927 - val_accuracy: 0.6961\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 762us/step - loss: 0.9589 - accuracy: 0.6748 - val_loss: 0.8937 - val_accuracy: 0.6978\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 757us/step - loss: 0.9653 - accuracy: 0.6712 - val_loss: 0.8945 - val_accuracy: 0.7006\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 758us/step - loss: 0.9547 - accuracy: 0.6757 - val_loss: 0.8968 - val_accuracy: 0.7011\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 753us/step - loss: 0.9752 - accuracy: 0.6781 - val_loss: 0.8955 - val_accuracy: 0.7050\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 759us/step - loss: 0.9755 - accuracy: 0.6702 - val_loss: 0.8934 - val_accuracy: 0.7017\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 755us/step - loss: 0.9759 - accuracy: 0.6771 - val_loss: 0.8947 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9579 - accuracy: 0.6755 - val_loss: 0.8916 - val_accuracy: 0.7028\n",
      "57/57 [==============================] - 0s 397us/step - loss: 0.8916 - accuracy: 0.7028\n",
      "approximate Test Accuracy: 70.28%\n"
     ]
    }
   ],
   "source": [
    "# define architecture \n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(X_t.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(8, activation='softmax')  # Output layer with 8 neurons for classification\n",
    "])\n",
    "\n",
    "# specify the way of training\n",
    "model.compile(optimizer='adam', # 'adam', 'sgd', 'rmsprop'\n",
    "              loss='sparse_categorical_crossentropy',  #  'sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_t, y_t, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'approximate Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c929389",
   "metadata": {},
   "source": [
    "## Superlearner\n",
    "Ensemble method of the learners described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b77b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a29992f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of base-models\n",
    "classifiers = {}\n",
    "classifiers['SGD_LM'] = SGDClassifier(max_iter=5000, alpha=0.001, loss='log_loss', penalty='l2')\n",
    "classifiers['logistic'] = LogisticRegression(C=1, penalty='l1', solver='liblinear')\n",
    "classifiers['Gradient_Boost'] = XGBRFClassifier(colsample_bynode=0.6, gamma=0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7a97865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect out of fold predictions form k-fold cross validation\n",
    "def out_of_fold_predictions(X, y, models):\n",
    "\t\"\"\"\n",
    "\tparam X: input data\n",
    "\tparam y: class labels\n",
    "\tmodels: based models for the super-learner\n",
    "\t\"\"\"\n",
    "\tX_out, y_out = list(), list()\n",
    "\t# define partition of data into 10 folds\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t# enumerate splits\n",
    "\tfor train_ids, test_ids in kfold.split(X):\n",
    "\t\ty_preds = list()\n",
    "\t\t# split data\n",
    "\t\tX = np.array(X)\n",
    "\t\tX_train, X_test = X[train_ids], X[test_ids]\n",
    "\t\ty = np.array(y)\n",
    "\t\ty_train, y_test = y[train_ids], y[test_ids]\n",
    "\t\ty_out.extend(y_test)\n",
    "\t\t# fit and make predictions with each base model\n",
    "\t\tfor _, model in models.items():\n",
    "\t\t\tmodel.fit(X_train, y_train)\n",
    "\t\t\ty_probs = model.predict_proba(X_test)\n",
    "\t\t\t# store columns\n",
    "\t\t\ty_preds.append(y_probs)\n",
    "\t\t# store fold yhats as columns\n",
    "\t\tX_out.append(np.hstack(y_preds))\n",
    "\treturn np.vstack(X_out), np.asarray(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e48cde51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_base(X, y, models):\n",
    "\tfor name, model in models.items():\n",
    "\t\tif name != 'MLP':\n",
    "\t\t\tmodel.fit(X, y)\n",
    "\n",
    "## used on out-of-fold predictions\n",
    "def fit_meta(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def superlearner_predict(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor _, model in models.items():\n",
    "\t\ty_probs = model.predict_proba(X)\n",
    "\t\tmeta_X.append(y_probs)\n",
    "\tmeta_X = np.hstack(meta_X)\n",
    "\treturn meta_model.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "807be515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Learner test accuracy: 69.222\n"
     ]
    }
   ],
   "source": [
    "# use LDA to reduce dimensions\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train_scaled, y_train)\n",
    "X_train_LDA = LDA.transform(X_train_scaled)\n",
    "# train-test split\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train_LDA, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "meta_X, meta_y = out_of_fold_predictions(X_t, y_t, classifiers)\n",
    "\n",
    "fit_base(X_t, y_t, classifiers)\n",
    "meta_model = fit_meta(meta_X, meta_y)\n",
    "\n",
    "y_pred = superlearner_predict(X_val, classifiers, meta_model)\n",
    "print('Super Learner test accuracy: %.3f' % (accuracy_score(y_val, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c8fe7",
   "metadata": {},
   "source": [
    "## Export in csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the predictions on the test data in csv format\n",
    "prediction = pd.DataFrame(y_pred, columns=['Genre'])\n",
    "prediction.index.name='Id'\n",
    "prediction.to_csv('myprediction.csv') # export to csv file\n",
    "\n",
    "# The csv file should be of the form\n",
    "#Id, Genre\n",
    "#0, Folk\n",
    "#1, Hip-Hop\n",
    "#2, International\n",
    "#...\n",
    "#1998, Experimental\n",
    "#1999, Pop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
