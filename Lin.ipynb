{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SML Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set basic parameters ###\n",
    "\n",
    "global_seed = 2023 # set to ensure reproducibility\n",
    "n_seeds = 3 # number of seeds used for estimating the accuracy\n",
    "split_rate = 0.2 # proportion of data used as test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (running this cell once is enough)\n",
    "\n",
    "### download required libraries ###\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "# List of required libraries\n",
    "required_libraries = ['numpy', 'matplotlib', 'seaborn', 'mrmr-selection', 'scikit-learn', 'xgboost']\n",
    "\n",
    "# Check if each required library is installed, and install it if not\n",
    "for library in required_libraries:\n",
    "    try:\n",
    "        # Try to import the library\n",
    "        importlib.import_module(library)\n",
    "    except ImportError:\n",
    "        # If the library is not installed, install it using pip\n",
    "        print(f\"{library} is not installed. Installing...\")\n",
    "        subprocess.check_call(['pip', 'install', library])\n",
    "        print(f\"{library} has been successfully installed\")\n",
    "\n",
    "### import required libraries ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# pre-processing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from mrmr import mrmr_classif\n",
    "# classifiers\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBRFClassifier\n",
    "# evaluations\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(global_seed)\n",
    "\n",
    "### load data \n",
    "X = pd.read_csv('data/X_train.csv', index_col = 0, header=[0, 1, 2]) # inputs \n",
    "y = pd.read_csv('data/y_train.csv', index_col = 0).squeeze('columns') # labels\n",
    "# total number of rows and columns(attributes)\n",
    "n, p = np.shape(X)\n",
    "\n",
    "### transform class labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "### check class labels\n",
    "# label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "# print(\"Label mapping:\", label_mapping)\n",
    "\n",
    "# use different seeds to test the algorithm\n",
    "seeds = np.random.randint(0, 2025, size=n_seeds)\n",
    "X_train_list = [None for _ in range(n_seeds)]\n",
    "X_test_list = [None for _ in range(n_seeds)]\n",
    "y_train_list = [None for _ in range(n_seeds)]\n",
    "y_test_list = [None for _ in range(n_seeds)]\n",
    "for i, seed in enumerate(seeds):\n",
    "    ### split into train and test set  \n",
    "    X_train_list[i], X_test_list[i], y_train_list[i], y_test_list[i] = train_test_split(X, y, test_size=split_rate, random_state=seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pre-process parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_method = 'standard' # 'standard', 'minmax', 'none'\n",
    "### Dimension reduction/feature selection\n",
    "reduction_method = 'igr' # 'igr', 'mrmr', 'chroma', 'pca', 'lda', 'vae', 'none'\n",
    "# number of components to keep\n",
    "n_components = 250 # will be ignored if reduction_method == 'none' or 'lda'\n",
    " # the chromagram selected\n",
    "chroma_choice = 'chroma_stft' # only necessary for reduction_method == 'chroma'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pre-processing\n",
    "X_train_list_scaled = [None for _ in range(n_seeds)]\n",
    "X_test_list_scaled = [None for _ in range(n_seeds)]\n",
    "if preprocess_method == 'standard':\n",
    "    for i in range(n_seeds):\n",
    "        scaler = StandardScaler()\n",
    "        X_train_list_scaled[i] = scaler.fit_transform(X_train_list[i])\n",
    "        X_test_list_scaled[i] = scaler.transform(X_test_list[i])\n",
    "elif preprocess_method == 'minmax':\n",
    "    for i in range(n_seeds):\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_list_scaled[i] = scaler.fit_transform(X_train_list[i])\n",
    "        X_test_list_scaled[i] = scaler.transform(X_test_list[i])\n",
    "elif preprocess_method == 'none':\n",
    "    X_train_list_scaled = X_train_list\n",
    "    X_test_list_scaled = X_test_list\n",
    "else:\n",
    "    raise Exception('preprocess_method must be of the standard, minmax, none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction/Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**igr(information gain ranking)**: selects the features with largest mutual information scores with the class labels  \n",
    "**mrmr(Max-Relevance, and Min-Redundancy)**: forward selection technique, uses mutual information(relevance), but will subtract the redundancy (defined as average mutual information between feature with already selected features)  \n",
    "**chroma**: simply select the one of three chromagrams and discard the other two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def reduce(X_train, X_test, y_train, columns):\n",
    "    \"\"\"\n",
    "    Reduce the X_train and X_test using specified method\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    if reduction_method == 'igr': ## information gain ranking algorithm \n",
    "        kbest = SelectKBest(score_func=mutual_info_classif, k=n_components)\n",
    "        kbest.fit(X_train, y_train)\n",
    "        selected_columns = X.columns[kbest.get_support()]\n",
    "\n",
    "        X_train_reduced = kbest.transform(X_train)\n",
    "        X_test_reduced = kbest.transform(X_test)\n",
    "    elif reduction_method == 'mrmr': ## Max-Relevance, and Min-Redundancy\n",
    "        X_train = pd.DataFrame(X_train, columns=columns)\n",
    "        X_test = pd.DataFrame(X_test, columns=columns)\n",
    "        selected_columns = mrmr_classif(X_train, \n",
    "                                    y_train, \n",
    "                                    K=n_components)\n",
    "        X_train_reduced = X_train[selected_columns]\n",
    "        X_test_reduced = X_test[selected_columns]\n",
    "    elif reduction_method == 'chroma':\n",
    "        chromas = {'chroma_cens', 'chroma_cqt', 'chroma_stft'}\n",
    "        removed_chromas = chromas.copy()\n",
    "        removed_chromas.remove(chroma_choice)\n",
    "        X_train_reduced = X_train[:, X.columns.get_level_values('feature').isin(removed_chromas) ]\n",
    "        X_test_reduced = X_test[:, X.columns.get_level_values('feature').isin(removed_chromas) ]\n",
    "    elif reduction_method == 'pca':\n",
    "        # perform the full PCA decomposition\n",
    "        pca = PCA(n_components=n_components)\n",
    "        # find the principal compoennts\n",
    "        X_train_reduced = pca.fit_transform(X_train)\n",
    "        X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "        ### check PCA explained variance ratio\n",
    "        # explained_variances = pca.explained_variance_ratio_\n",
    "        # plt.plot(range(1, n_components+1), explained_variances[:n_components])\n",
    "        # plt.title('explained variances by principal components')\n",
    "        # plt.xlabel('PC index')\n",
    "        # plt.ylabel('ratio of explained variance')\n",
    "    elif reduction_method == 'lda':\n",
    "        LDAclassifier = LinearDiscriminantAnalysis(n_components=7) # 8 classes in total \n",
    "        LDAclassifier.fit(X_train, y_train)\n",
    "\n",
    "        X_train_reduced = LDAclassifier.transform(X_train)\n",
    "        X_test_reduced = LDAclassifier.transform(X_test)\n",
    "    else:\n",
    "        raise Exception('reduction_method not implemented, please check the first cell for list of possible reduction methods')\n",
    "    end_time = time.time()\n",
    "\n",
    "    return X_train_reduced, X_test_reduced, (end_time - start_time)\n",
    "    \n",
    "reduction_times = [None for _ in range(n_seeds)]\n",
    "\n",
    "if reduction_method != 'none':\n",
    "    X_train_list_reduced = [None for _ in range(n_seeds)]\n",
    "    X_test_list_reduced = [None for _ in range(n_seeds)]\n",
    "\n",
    "    for i in range(n_seeds):\n",
    "        X_train_list_reduced[i], X_test_list_reduced[i], reduction_times[i] = reduce(X_train_list_scaled[i], X_test_list_scaled[i], \n",
    "                                                                 y_train_list[i], columns=X.columns)\n",
    "else:\n",
    "    X_train_list_reduced = X_train_list_scaled\n",
    "    X_test_list_reduced = X_test_list_scaled\n",
    "\n",
    "\n",
    "###  count the number of features selected from each group of features (used for the feature selection methods)\n",
    "# features = np.unique(X.columns.get_level_values('feature'))\n",
    "# category_counts = {}\n",
    "# for feature in features:\n",
    "#     category_counts[feature] = np.sum(selected_columns.get_level_values('feature') == feature)\n",
    "# category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters here, and click \"execute cell and below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training parameters\n",
    "classifier = 'SVM' # 'SVM', 'KNN', 'NB', 'LR', 'RF', 'XGBRF' \n",
    "# parameters for kernelSVM (support vector machine)\n",
    "params_SVM = {\n",
    "    'kernel': 'rbf', # the kernel used\n",
    "    'C': 3.0, # regularisation strength is 1/C \n",
    "    'gamma': 'scale' # the scale of rbf and poly kernels\n",
    "} \n",
    "# parameters for k-nearest neighbours\n",
    "params_KNN = {\n",
    "    'n_neighbors': 10, # the number of neighbours selected\n",
    "    'algorithm' : 'auto' # Algorithm used to compute the nearest neighbors: 'auto', 'ball_tree'. 'kd_tree', 'brute'\n",
    "} \n",
    "# parameters for logistic regression\n",
    "params_LR = {\n",
    "    'solver' : 'lbfgs', # optimiser used: 'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'\n",
    "    'C': 1, # regularisation strength is 1/C \n",
    "    'penalty' : 'l2' # 'l1', 'l2', 'elasticnet'\n",
    "}\n",
    "# parameters for random forest\n",
    "params_RF = {\n",
    "    'n_estimators' : 200, # number of trees in the forest\n",
    "    'max_depth': 5, # maximum tree depth\n",
    "    'min_samples_split': 5, # minimum size of node allowed for splitting\n",
    "    'min_samples_leaf': 3,  # minimum leaf node size \n",
    "    'max_features': 'auto' # number of features used in each tree, one of 'auto', 'sqrt', 'log2' or None(use all features)\n",
    "}\n",
    "# parameters for gradient boosting random forest\n",
    "params_XGBRF = {\n",
    "    'learning_rate': 0.01, # step size shrinkage used in each boosting iteration\n",
    "    'n_estimators': 100, # number of trees\n",
    "    'max_depth': 7, # max tree depth\n",
    "    'subsample': 0.6, # fraction of samples used for fitting each tree\n",
    "    'colsample_bynode': 0.8, # fraction of features used for fitting each tree\n",
    "    'gamma': 0.01, # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "}\n",
    "\n",
    "params_dict = {\n",
    "    'SVM' : params_SVM,\n",
    "    'KNN' : params_KNN,\n",
    "    'RF' : params_RF,\n",
    "    'XGBRF' : params_XGBRF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the model and record the training time\n",
    "\n",
    "train_times = [None for _ in range(n_seeds)]\n",
    "models = [None for _ in range(n_seeds)]\n",
    "\n",
    "if classifier == 'SVM':\n",
    "    for i in range(n_seeds):\n",
    "        models[i] = SVC(random_state=seed, **params_SVM) \n",
    "        start = time.time()\n",
    "        models[i].fit(X_train_list_reduced[i], y_train_list[i])\n",
    "        end = time.time()\n",
    "        train_times[i] = end - start\n",
    "elif classifier == 'KNN':\n",
    "    for i in range(n_seeds):\n",
    "        models[i] = KNeighborsClassifier(**params_KNN)\n",
    "        start = time.time()\n",
    "        models[i].fit(X_train_list_reduced[i], y_train_list[i])\n",
    "        end = time.time()\n",
    "        train_times[i] = end - start\n",
    "elif classifier == 'NB':\n",
    "    for i in range(n_seeds):\n",
    "        models[i] = GaussianNB()\n",
    "        start = time.time()\n",
    "        models[i].fit(X_train_list_reduced[i], y_train_list[i])\n",
    "        end = time.time()\n",
    "        train_times[i] = end - start\n",
    "elif classifier == 'LR':\n",
    "    for i in range(n_seeds):\n",
    "        models[i] = LogisticRegression(random_state=global_seed, **params_LR)\n",
    "        start = time.time()\n",
    "        models[i].fit(X_train_list_reduced[i], y_train_list[i])\n",
    "        end = time.time()\n",
    "        train_times[i] = end - start\n",
    "elif classifier == 'RF':\n",
    "    for i in range(n_seeds):\n",
    "        models[i] = RandomForestClassifier(random_state=global_seed, **params_RF)\n",
    "        start = time.time()\n",
    "        models[i].fit(X_train_list_reduced[i], y_train_list[i])\n",
    "        end = time.time()\n",
    "        train_times[i] = end - start\n",
    "elif classifier == 'XGBRF':\n",
    "    for i in range(n_seeds):\n",
    "        models[i] = XGBRFClassifier(random_state=global_seed, **params_XGBRF)\n",
    "        start = time.time()\n",
    "        models[i].fit(X_train_list_reduced[i], y_train_list[i])\n",
    "        end = time.time()\n",
    "        train_times[i] = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_times = [None for _ in range(n_seeds)]\n",
    "test_accs = [None for _ in range(n_seeds)]\n",
    "gen_gaps = [None for _ in range(n_seeds)]\n",
    "test_f1s = [None for _ in range(n_seeds)]\n",
    "conf_mats = [None for _ in range(n_seeds)]\n",
    "\n",
    "for i in range(n_seeds):\n",
    "    start = time.time()\n",
    "    y_pred = models[i].predict(X_test_list_reduced[i])\n",
    "    end = time.time()\n",
    "    predict_times[i] = end - start\n",
    "\n",
    "    y_pred_train = models[i].predict(X_train_list_reduced[i])\n",
    "\n",
    "    train_acc = accuracy_score(y_train_list[i], y_pred_train)\n",
    "    test_acc = accuracy_score(y_test_list[i], y_pred)\n",
    "    test_accs[i] = test_acc\n",
    "    gen_gaps[i] = train_acc - test_acc\n",
    "    test_f1s[i] = f1_score(y_test_list[i], y_pred, average='micro')\n",
    "    conf_mats[i] = confusion_matrix(y_test_list[i], y_pred)\n",
    "\n",
    "print(f'-------- Current classifier : {classifier} ---------')\n",
    "print(f'parameters: {params_dict[classifier]}')\n",
    "print(f'test set accuracy {np.mean(test_accs)} +- {np.std(test_accs)}')\n",
    "print(f'average generalisation gap {np.mean(gen_gaps)}')\n",
    "print(f'average test set f1 score {np.mean(test_f1s)}')\n",
    "print(f'average feature reduction/selection time {np.mean(reduction_times)}')\n",
    "print(f'average training time {np.mean(train_times)}')\n",
    "print(f'average prediction time {np.mean(predict_times)}')\n",
    "\n",
    "conf_mats = np.array(conf_mats)\n",
    "avg_conf_mat = np.mean(conf_mats, axis=0) # average confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix, title=''):\n",
    "    \"\"\" Plot the confusion matrix \"\"\"\n",
    "    axis_labels = label_encoder.classes_\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(matrix, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='matched pairs')\n",
    "    plt.title(f'Plot of confusion matrix {title}')\n",
    "    plt.xticks(np.arange(len(axis_labels)), axis_labels, rotation=45)\n",
    "    plt.yticks(np.arange(len(axis_labels)), axis_labels)\n",
    "    plt.xlabel('genre')\n",
    "    plt.ylabel('genre')\n",
    "    plt.show()\n",
    "\n",
    "plot_heatmap(avg_conf_mat, f'classifier: {classifier}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
