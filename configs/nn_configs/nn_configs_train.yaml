
# mlp:
#   input_dim: [518]
#   hidden_dims: [[16], [32, 16]]
#   output_dim: [8]
#   dropout: [0.0, 0.2]
#   activation: ['relu', 'tanh']
#   n_groups: [11]
#   n_features_per_group: [[84, 84, 84, 140, 7, 7, 7, 49, 7, 42, 7]]
#   n_embed_dim: [6, 7, 8]
#   use_patch_embedding: [true, false]


# transformer:
#   input_dim: [518]
#   n_heads: [2]
#   n_groups: [11]
#   n_features_per_group: [[84, 84, 84, 140, 7, 7, 7, 49, 7, 42, 7]]
#   n_embed_dim: [32, 64]
#   dim_feedforward: [128]
#   dropout: [0.0, 0.5]
#   output_dim: [8]
#   num_layers: [3]
#   use_patch_embedding: [true]

# transformer:
#   input_dim: [518]
#   n_heads: [1]
#   n_groups: [11]
#   n_features_per_group: [[84, 84, 84, 140, 7, 7, 7, 49, 7, 42, 7]]
#   n_embed_dim: [8]
#   dim_feedforward: [64, 128, 256, 512]
#   dropout: [0.0, 0.5, 0.7]
#   output_dim: [8]
#   num_layers: [1, 2, 3, 10]
#   use_patch_embedding: [false]


resnet:
  input_dim: [350]
  n_groups: [11]
  n_features_per_group: [[84, 84, 84, 140, 7, 7, 7, 49, 7, 42, 7]]
  n_embed_dim: [32]
  dropout: [0.5, 0.7]
  output_dim: [8]
  block_size: [8, 16, 32, 64, 128]
  num_blocks: [3, 5, 10, 20, 50]
  use_patch_embedding: [false]



