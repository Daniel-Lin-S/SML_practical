
# mlp:
#   input_dim: [518]
#   hidden_dims: [[16], [32, 16]]
#   output_dim: [8]
#   dropout: [0.0, 0.2]
#   activation: ['relu', 'tanh']
#   n_groups: [11]
#   n_features_per_group: [[84, 84, 84, 140, 7, 7, 7, 49, 7, 42, 7]]
#   n_embed_dim: [6, 7, 8]
#   use_patch_embedding: [true, false]


transformer:
  input_dim: [518]
  n_heads: [2]
  n_groups: [11]
  n_features_per_group: [[84, 84, 84, 140, 7, 7, 7, 49, 7, 42, 7]]
  n_embed_dim: [32, 64]
  dim_feedforward: [128]
  dropout: [0.0, 0.5]
  output_dim: [8]
  num_layers: [3]
  use_patch_embedding: [true]



