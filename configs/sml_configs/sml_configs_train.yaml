# naive_bayes:

# logistic_regression:
#   C: [0.001, 0.01, 0.1, 1, 10, 100] # regularisation strength
#   penalty: ['l2', 'none']
#   solver: ['lbfgs']  # solver for optimisation

# xgboost_rf:
#   # learning_rate: [0.1]
#   # subsample: [0.5]
#   # colsample_bynode: [0.2]
#   learning_rate: [0.05] # step size shrinkage used in each boosting iteration
#   n_estimators: [100, 200] # number of trees
#   max_depth: [5] # max tree depth
#   subsample: [0.3, 0.5, 0.9] # fraction of samples used for fitting each tree
#   colsample_bynode: [0.5, 0.6, 0.9] # fraction of features used for fitting each tree
#   # gamma: [0, 0.1, 0.2] # Minimum loss reduction required to make a further partition on a leaf node of the tree.

# adaboost:
#   n_estimators: [50, 100, 200]
#   learning_rate: [0.01, 0.1, 0.5]
  # max_depth: [2, 4, 6]

# l_svm:
#   alpha: [0.0001, 0.001, 0.01, 1]
#   penalty: ['l1', 'l2', 'elasticnet']
#   loss: ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']


c_svm:
  C: [0.001, 0.01, 0.1, 1] # regularisation strength
  # kernel: ['linear', 'poly', 'rbf', 'sigmoid'] # kernel function
  kernel: ['poly']
  degree: [2, 3, 4, 5, 6] # degree of the polynomial kernel function
  gamma: ['scale', 'auto'] # kernel coefficient for 'rbf', 'poly', and 'sigmoid'
  
