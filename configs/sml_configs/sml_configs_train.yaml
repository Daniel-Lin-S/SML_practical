# naive_bayes:

# logistic_regression:
#   C: [0.001, 0.01, 0.1, 1, 10, 100] # regularisation strength
#   penalty: ['l2', 'none']
#   solver: ['lbfgs']  # solver for optimisation

xgboost_rf:
  # learning_rate: [0.1]
  # subsample: [0.5]
  # colsample_bynode: [0.2]
  xgboost_rf__learning_rate: [0.05] # step size shrinkage used in each boosting iteration
  xgboost_rf__n_estimators: [100, 200] # number of trees
  xgboost_rf__max_depth: [5] # max tree depth
  xgboost_rf__subsample: [0.3, 0.5, 0.9] # fraction of samples used for fitting each tree
  xgboost_rf__colsample_bynode: [0.5, 0.6, 0.9] # fraction of features used for fitting each tree
  # xgboost_rf__gamma: [0, 0.1, 0.2] # Minimum loss reduction required to make a further partition on a leaf node of the tree.

random_forest:
  random_forest__n_estimators: [100, 200, 300] # number of trees in the forest
  random_forest__max_depth: [None, 5, 10] # maximum depth of the tree
  random_forest__min_samples_split: [2, 5, 10] # minimum number of samples required to split an internal node
  random_forest__min_samples_leaf: [1, 2, 4] # minimum number of samples required to be at a leaf node
  random_forest__max_features: ['auto', 'sqrt'] # number of features to consider when looking for the best split


# adaboost:
#   n_estimators: [50, 100, 200]
#   learning_rate: [0.01, 0.1, 0.5]
  # max_depth: [2, 4, 6]

# l_svm:
#   alpha: [0.0001, 0.001, 0.01, 1]
#   penalty: ['l1', 'l2', 'elasticnet']
#   loss: ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']


c_svm:
  # C: [0.001, 0.01, 0.1, 1] # regularisation strength
  c_svm__C: [0.1, 1.0, 2.0]
  # kernel: ['linear', 'poly', 'rbf', 'sigmoid'] # kernel function
  c_svm__kernel: ['poly', 'rbf']
  degree: [2, 3, 4, 5, 6] # degree of the polynomial kernel function
  # c_svm__degree: [3]
  c_svm__gamma: ['scale', 'auto'] # kernel coefficient for 'rbf', 'poly', and 'sigmoid'
  
